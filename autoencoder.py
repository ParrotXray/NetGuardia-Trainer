"""
Autoencoder è¨“ç·´éšæ®µï¼ˆæœ€çµ‚æ”¹è‰¯ç‰ˆ - ä¿®æ­£é›¢ç¾¤é»éæ¿¾ï¼‰ï¼š
- ä¿®æ­£é›¢ç¾¤é»éæ¿¾é‚è¼¯ï¼Œé¿å…åˆªé™¤æ‰€æœ‰æ¨£æœ¬
- é€æ¬„ä½éæ¿¾ï¼Œè€Œéè¦æ±‚æ‰€æœ‰æ¬„ä½éƒ½ç¬¦åˆ
"""
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from tensorflow.keras import layers, models, regularizers
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.optimizers import Adam
import tensorflow as tf
import joblib
import matplotlib.pyplot as plt
from scipy import stats

print("=" * 50)
print("ğŸ¤– Step 2: Autoencoder è¨“ç·´ï¼ˆæœ€çµ‚æ”¹è‰¯ç‰ˆ v2ï¼‰")
print("=" * 50)

print("TensorFlow version:", tf.__version__)
print("GPUs:", tf.config.list_physical_devices('GPU'))

# === 1ï¸âƒ£ è®€è³‡æ–™ ===
df = pd.read_csv("output_anomaly.csv")
df.columns = df.columns.str.strip()

print(f"âœ… è¼‰å…¥è³‡æ–™: {df.shape}")

# === 2ï¸âƒ£ ä¿å­˜æ¨™ç±¤ ===
labels = df['Label'].copy()
print(f"ğŸ“‹ æ¨™ç±¤åˆ†å¸ƒ:\n{labels.value_counts()}")

# === 3ï¸âƒ£ åªç”¨ BENIGN è¨“ç·´ Autoencoder ===
df_benign = df[df['Label'] == 'BENIGN'].copy()

# ç§»é™¤æ‰€æœ‰éç‰¹å¾µæ¬„ä½
exclude_cols = ['Label', 'anomaly_if']
X_train = df_benign.drop(columns=exclude_cols, errors='ignore')
X_train = X_train.select_dtypes(include=[np.number])

print(f"âœ… BENIGN æ¨£æœ¬æ•¸ï¼ˆè™•ç†å‰ï¼‰: {len(X_train)} / {len(df)}")
print(f"ğŸ”¢ ç‰¹å¾µç¶­åº¦: {X_train.shape}")

# === 4ï¸âƒ£ æ¸…ç†æ•¸å€¼ ===
X_train = X_train.replace([np.inf, -np.inf], np.nan).fillna(0)
X_train = np.clip(X_train, -1e9, 1e9)

# === ğŸ†• 5ï¸âƒ£ æ”¹è‰¯çš„é›¢ç¾¤é»è™•ç†ï¼ˆå¹³è¡¡ç‰ˆï¼‰===
print("\nğŸ” ç§»é™¤ BENIGN é›¢ç¾¤é»ï¼ˆå¹³è¡¡ç‰ˆ v4ï¼‰...")

# å…ˆå‚™ä»½
X_train_backup = X_train.copy()

# ç­–ç•¥ï¼šåªä½¿ç”¨æ•´é«” MSE + æ¥µç«¯å€¼é›™é‡éæ¿¾ï¼ˆä¸ç”¨ IQRï¼‰
# è¨ˆç®—æ¯å€‹æ¨£æœ¬çš„æ¨™æº–åŒ–å¾Œå¹³æ–¹èª¤å·®ç¸½å’Œ
X_train_normalized = (X_train - X_train.mean()) / (X_train.std() + 1e-8)
sample_mse = (X_train_normalized ** 2).sum(axis=1)

# æ–¹æ³• 1: ç§»é™¤ MSE æœ€é«˜çš„ 3% æ¨£æœ¬ï¼ˆæº«å’Œï¼‰
mse_threshold = sample_mse.quantile(0.97)
mse_mask = sample_mse < mse_threshold

print(f"  ğŸ“Š MSE éæ¿¾: ç§»é™¤å‰ 3% é«˜ MSE æ¨£æœ¬")
print(f"     MSE é–€æª»: {mse_threshold:.2f}")
print(f"     ä¿ç•™: {mse_mask.sum()} / {len(X_train)}")

# æ–¹æ³• 2: åªç§»é™¤æœ‰æ¥µç«¯æ¥µç«¯å€¼çš„æ¨£æœ¬ï¼ˆ0.05% å’Œ 99.95%ï¼‰
extreme_mask = pd.Series([True] * len(X_train), index=X_train.index)
extreme_cols = []

for col in X_train.columns:
    # åªé‡å°çœŸæ­£çš„æ¥µç«¯å€¼
    lower_extreme = X_train[col].quantile(0.0005)
    upper_extreme = X_train[col].quantile(0.9995)

    col_extreme = (X_train[col] < lower_extreme) | (X_train[col] > upper_extreme)

    if col_extreme.sum() > 0:
        extreme_mask = extreme_mask & ~col_extreme
        extreme_cols.append(col)

print(f"  ğŸ“Š æ¥µç«¯å€¼éæ¿¾: {len(extreme_cols)} å€‹æ¬„ä½æœ‰æ¥µç«¯å€¼")
print(f"     ä¿ç•™: {extreme_mask.sum()} / {len(X_train)}")

# çµåˆå…©ç¨®æ–¹æ³•ï¼ˆOR é‚è¼¯ï¼šä»»ä¸€æ–¹æ³•èªç‚ºæ­£å¸¸å³ä¿ç•™ï¼‰
# åªç§»é™¤å…©å€‹æ–¹æ³•éƒ½èªç‚ºæ˜¯é›¢ç¾¤é»çš„æ¨£æœ¬
final_mask = mse_mask | extreme_mask

X_train_clean = X_train[final_mask]

outliers_removed = len(X_train) - len(X_train_clean)
outlier_ratio = outliers_removed / len(X_train)

print(f"  âŒ ç§»é™¤é›¢ç¾¤é»: {outliers_removed} ({outlier_ratio:.2%})")
print(f"  âœ… ä¿ç•™æ¨£æœ¬æ•¸: {len(X_train_clean)}")

# å®‰å…¨æª¢æŸ¥ï¼šå¦‚æœç§»é™¤å¤ªå¤šï¼ˆ>10%ï¼‰æˆ–å¤ªå°‘æ¨£æœ¬ï¼Œä½¿ç”¨åŸå§‹è³‡æ–™
if outlier_ratio > 0.10:
    print(f"  âš ï¸ é›¢ç¾¤é»æ¯”ä¾‹éé«˜ ({outlier_ratio:.2%})ï¼Œä½¿ç”¨åŸå§‹è³‡æ–™")
    X_train = X_train_backup
elif outlier_ratio < 0.005:  # æ”¹æˆ 0.5%
    print(f"  â„¹ï¸ é›¢ç¾¤é»æ¥µå°‘ ({outlier_ratio:.2%})ï¼Œä½¿ç”¨åŸå§‹è³‡æ–™")
    X_train = X_train_backup
elif len(X_train_clean) < 10000:  # è‡³å°‘ä¿ç•™ 1 è¬å€‹æ¨£æœ¬
    print(f"  âš ï¸ ä¿ç•™æ¨£æœ¬æ•¸å¤ªå°‘ ({len(X_train_clean)})ï¼Œä½¿ç”¨åŸå§‹è³‡æ–™")
    X_train = X_train_backup
else:
    print(f"  âœ… é›¢ç¾¤é»è™•ç†æˆåŠŸ")
    X_train = X_train_clean

print(f"\nğŸ“Š æœ€çµ‚è¨“ç·´æ¨£æœ¬æ•¸: {len(X_train)}")

# === 6ï¸âƒ£ æ¨™æº–åŒ– ===
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)

print(f"âœ… æ¨™æº–åŒ–å®Œæˆï¼Œå½¢ç‹€: {X_train_scaled.shape}")

# === 7ï¸âƒ£ å»ºç«‹ Autoencoder ===
input_dim = X_train_scaled.shape[1]
print(f"\nğŸ§© Autoencoder è¼¸å…¥ç¶­åº¦: {input_dim}")

input_layer = layers.Input(shape=(input_dim,))

# Encoder
encoded = layers.Dense(256, activation='relu',
                      kernel_regularizer=regularizers.l2(0.001))(input_layer)
encoded = layers.BatchNormalization()(encoded)
encoded = layers.Dropout(0.2)(encoded)

encoded = layers.Dense(128, activation='relu',
                      kernel_regularizer=regularizers.l2(0.001))(encoded)
encoded = layers.BatchNormalization()(encoded)
encoded = layers.Dropout(0.2)(encoded)

encoded = layers.Dense(64, activation='relu',
                      kernel_regularizer=regularizers.l2(0.001))(encoded)
encoded = layers.BatchNormalization()(encoded)
encoded = layers.Dropout(0.1)(encoded)

# Bottleneck
bottleneck = layers.Dense(8, activation='relu',
                         kernel_regularizer=regularizers.l2(0.001))(encoded)

# Decoder
decoded = layers.Dense(64, activation='relu',
                      kernel_regularizer=regularizers.l2(0.001))(bottleneck)
decoded = layers.BatchNormalization()(decoded)
decoded = layers.Dropout(0.1)(decoded)

decoded = layers.Dense(128, activation='relu',
                      kernel_regularizer=regularizers.l2(0.001))(decoded)
decoded = layers.BatchNormalization()(decoded)
decoded = layers.Dropout(0.2)(decoded)

decoded = layers.Dense(256, activation='relu',
                      kernel_regularizer=regularizers.l2(0.001))(decoded)
decoded = layers.BatchNormalization()(decoded)
decoded = layers.Dropout(0.2)(decoded)

output_layer = layers.Dense(input_dim, activation='linear')(decoded)

autoencoder = models.Model(inputs=input_layer, outputs=output_layer)

optimizer = Adam(learning_rate=0.001)
autoencoder.compile(optimizer=optimizer, loss='mse')

print("\nğŸ“ æ¨¡å‹çµæ§‹:")
autoencoder.summary()

# === 8ï¸âƒ£ è¨­å®š Callbacks ===
reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=5,
    min_lr=1e-7,
    verbose=1
)

early_stop = EarlyStopping(
    monitor='val_loss',
    patience=10,
    restore_best_weights=True,
    verbose=1
)

# === 9ï¸âƒ£ è¨“ç·´æ¨¡å‹ ===
print("\nğŸš€ é–‹å§‹è¨“ç·´...")
history = autoencoder.fit(
    X_train_scaled, X_train_scaled,
    epochs=100,
    batch_size=512,
    validation_split=0.15,
    callbacks=[early_stop, reduce_lr],
    shuffle=True,
    verbose=1
)

print(f"\nâœ… è¨“ç·´å®Œæˆï¼Œå¯¦éš›è¨“ç·´ {len(history.history['loss'])} å€‹ epoch")

# === ğŸ”Ÿ ç”¨æ•´ä»½è³‡æ–™åšé‡å»ºèª¤å·® ===
print("\nğŸ” è¨ˆç®—å…¨éƒ¨è³‡æ–™çš„é‡å»ºèª¤å·®...")

df_all = df.drop(columns=exclude_cols, errors='ignore')
X_all = df_all.select_dtypes(include=[np.number])
X_all = X_all[X_train.columns]
X_all = X_all.replace([np.inf, -np.inf], np.nan).fillna(0)
X_all = np.clip(X_all, -1e9, 1e9)

X_all_scaled = scaler.transform(X_all)

recon = autoencoder.predict(X_all_scaled, verbose=0)
mse = np.mean(np.square(X_all_scaled - recon), axis=1)

# === 11ï¸âƒ£ è©³ç´°è¨ºæ–· ===
print("\n" + "=" * 50)
print("ğŸ” é‡å»ºèª¤å·®è¨ºæ–·")
print("=" * 50)

mse_benign = mse[labels == 'BENIGN']
mse_attack = mse[labels != 'BENIGN']

print(f"\nğŸ“Š BENIGN æ¨£æœ¬ MSE:")
print(f"  - Mean: {mse_benign.mean():.6f}")
print(f"  - Std:  {mse_benign.std():.6f}")
print(f"  - Min:  {mse_benign.min():.6f}")
print(f"  - Max:  {mse_benign.max():.6f}")
print(f"  - Median: {np.median(mse_benign):.6f}")
print(f"  - 95th percentile: {np.percentile(mse_benign, 95):.6f}")
print(f"  - 99th percentile: {np.percentile(mse_benign, 99):.6f}")

print(f"\nğŸš¨ Attack æ¨£æœ¬ MSE:")
print(f"  - Mean: {mse_attack.mean():.6f}")
print(f"  - Std:  {mse_attack.std():.6f}")
print(f"  - Min:  {mse_attack.min():.6f}")
print(f"  - Max:  {mse_attack.max():.6f}")
print(f"  - Median: {np.median(mse_attack):.6f}")

print(f"\nğŸ“ˆ MSE æ¯”å€¼ (Attack/BENIGN):")
if mse_benign.mean() > 0:
    print(f"  - Mean æ¯”å€¼: {mse_attack.mean() / mse_benign.mean():.2f}x")
if mse_benign.max() > 0:
    print(f"  - Max æ¯”å€¼: {mse_attack.max() / mse_benign.max():.2f}x")
print(f"  - Median æ¯”å€¼: {np.median(mse_attack) / np.median(mse_benign):.2f}x")

print(f"\nğŸ¯ å„æ”»æ“Šé¡å‹ MSE:")
for attack_type in sorted(labels[labels != 'BENIGN'].unique()):
    mse_type = mse[labels == attack_type]
    count = len(mse_type)
    print(f"  {attack_type:20s}: Mean={mse_type.mean():.6f}, "
          f"Median={np.median(mse_type):.6f}, "
          f"Max={mse_type.max():.6f}, Count={count}")

# === 12ï¸âƒ£ å¤šç¨®é–€æª»ç­–ç•¥ ===
print("\n" + "=" * 50)
print("ğŸ¯ é–€æª»ç­–ç•¥æ¯”è¼ƒ")
print("=" * 50)

thresholds = {}

for p in [75, 80, 85, 90, 95, 99]:
    thresholds[f"All_P{p}"] = np.percentile(mse, p)

for p in [85, 90, 95, 97, 99, 99.5]:
    thresholds[f"BENIGN_P{p}"] = np.percentile(mse_benign, p)

for n in [2, 2.5, 3, 3.5]:
    thresholds[f"BENIGN_M+{n}S"] = mse_benign.mean() + n * mse_benign.std()

for n in [2, 3, 4, 5]:
    median = np.median(mse_benign)
    mad = np.median(np.abs(mse_benign - median))
    thresholds[f"BENIGN_Med+{n}MAD"] = median + n * mad

print(f"\n{'ç­–ç•¥':<20} {'é–€æª»å€¼':<12} {'åµæ¸¬æ”»æ“Š':<15} {'èª¤å ±':<10} {'åµæ¸¬ç‡':<10} {'Precision':<10} {'F1':<10}")
print("-" * 100)

best_strategy = None
best_f1 = 0
all_results = []

for name, threshold in sorted(thresholds.items(), key=lambda x: x[1]):
    is_anomaly = (mse > threshold).astype(int)

    tp = ((labels != 'BENIGN') & (is_anomaly == 1)).sum()
    fp = ((labels == 'BENIGN') & (is_anomaly == 1)).sum()
    fn = ((labels != 'BENIGN') & (is_anomaly == 0)).sum()
    tn = ((labels == 'BENIGN') & (is_anomaly == 0)).sum()

    total_attacks = (labels != 'BENIGN').sum()
    detection_rate = tp / total_attacks if total_attacks > 0 else 0

    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = detection_rate
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

    print(f"{name:<20} {threshold:<12.6f} {tp:>6}/{total_attacks:<7} {fp:<10} "
          f"{detection_rate:>8.2%}  {precision:>8.4f}  {f1:>8.4f}")

    all_results.append({
        'strategy': name,
        'threshold': threshold,
        'tp': tp, 'fp': fp, 'fn': fn, 'tn': tn,
        'detection_rate': detection_rate,
        'precision': precision,
        'f1': f1
    })

    if f1 > best_f1:
        best_f1 = f1
        best_strategy = name
        best_threshold = threshold
        best_results = all_results[-1]

print(f"\nğŸ† æ¨è–¦ç­–ç•¥: {best_strategy}")
print(f"ğŸ¯ æ¨è–¦é–€æª»: {best_threshold:.6f}")
print(f"ğŸ“Š F1-Score: {best_f1:.4f}")

# === 13ï¸âƒ£ ä½¿ç”¨æ¨è–¦é–€æª» ===
threshold = best_threshold
is_anomaly = (mse > threshold).astype(int)

print(f"\nğŸ“Š ä½¿ç”¨æ¨è–¦é–€æª»çš„çµæœ:")
print(f"  - åµæ¸¬åˆ°ç•°å¸¸: {is_anomaly.sum()} / {len(df)}")

# === 14ï¸âƒ£ è¼¸å‡ºçµæœ ===
output = X_all.copy()
output['anomaly_score'] = mse
output['is_anomaly'] = is_anomaly
output['Label'] = labels.values

output.to_csv("output_autoencoder.csv", index=False)
print("\nğŸ’¾ å·²è¼¸å‡º: output_autoencoder.csv")

# === 15ï¸âƒ£ å„²å­˜æ¨¡å‹ ===
autoencoder.save("autoencoder_cic_model.h5")
joblib.dump(scaler, "scaler_ae.pkl")
joblib.dump({
    'threshold': threshold,
    'strategy': best_strategy,
    'all_thresholds': thresholds,
    'best_results': best_results
}, "threshold_info.pkl")

print("âœ… å·²ä¿å­˜æ¨¡å‹å’Œé–€æª»è³‡è¨Š")

# === 16ï¸âƒ£ è¦–è¦ºåŒ–ï¼ˆç°¡åŒ–ç‰ˆï¼Œ9 å¼µåœ–ï¼‰===
fig = plt.figure(figsize=(20, 12))

# 1. è¨“ç·´æå¤±
ax1 = plt.subplot(3, 3, 1)
ax1.plot(history.history['loss'], label='Train', linewidth=2)
ax1.plot(history.history['val_loss'], label='Val', linewidth=2)
ax1.set_title('Training Loss')
ax1.legend()
ax1.grid(True, alpha=0.3)

# 2. MSE åˆ†ä½ˆ
ax2 = plt.subplot(3, 3, 2)
ax2.hist(mse_benign, bins=100, alpha=0.7, label='BENIGN', color='green', density=True)
ax2.hist(mse_attack, bins=100, alpha=0.7, label='Attack', color='red', density=True)
ax2.axvline(threshold, color='black', linestyle='--', linewidth=2)
ax2.set_xlabel('MSE')
ax2.set_title('MSE Distribution')
ax2.legend()
ax2.grid(True, alpha=0.3)

# 3. MSE åˆ†ä½ˆï¼ˆæ”¾å¤§ï¼‰
ax3 = plt.subplot(3, 3, 3)
max_display = np.percentile(mse, 98)
ax3.hist(mse_benign[mse_benign < max_display], bins=100, alpha=0.7, label='BENIGN', color='green', density=True)
ax3.hist(mse_attack[mse_attack < max_display], bins=100, alpha=0.7, label='Attack', color='red', density=True)
ax3.axvline(threshold, color='black', linestyle='--', linewidth=2)
ax3.set_xlabel('MSE')
ax3.set_title(f'MSE Distribution (Zoom < {max_display:.2f})')
ax3.legend()
ax3.grid(True, alpha=0.3)

# 4. å„æ”»æ“Šé¡å‹ MSE
ax4 = plt.subplot(3, 3, 4)
attack_types = sorted(labels[labels != 'BENIGN'].unique())
mse_by_type = [mse_benign] + [mse[labels == at] for at in attack_types]
labels_plot = ['BENIGN'] + list(attack_types)
bp = ax4.boxplot(mse_by_type, labels=labels_plot, patch_artist=True)
ax4.axhline(threshold, color='red', linestyle='--', linewidth=2)
ax4.set_ylabel('MSE')
ax4.set_title('MSE by Type')
ax4.set_xticklabels(labels_plot, rotation=45, ha='right')
ax4.grid(True, alpha=0.3, axis='y')

# 5. F1-Score æ¯”è¼ƒ
ax5 = plt.subplot(3, 3, 5)
top10 = sorted(all_results, key=lambda x: x['f1'], reverse=True)[:10]
names = [r['strategy'] for r in top10]
f1s = [r['f1'] for r in top10]
colors = ['gold' if r['strategy'] == best_strategy else 'steelblue' for r in top10]
ax5.barh(names, f1s, color=colors)
ax5.set_xlabel('F1-Score')
ax5.set_title('Top 10 Strategies')
ax5.grid(True, alpha=0.3, axis='x')

# 6. æ··æ·†çŸ©é™£
ax6 = plt.subplot(3, 3, 6)
cm = np.array([[best_results['tn'], best_results['fp']],
               [best_results['fn'], best_results['tp']]])
im = ax6.imshow(cm, cmap='Blues')
ax6.set_xticks([0, 1])
ax6.set_yticks([0, 1])
ax6.set_xticklabels(['Pred Normal', 'Pred Attack'])
ax6.set_yticklabels(['True Normal', 'True Attack'])
for i in range(2):
    for j in range(2):
        ax6.text(j, i, f'{cm[i, j]:,}', ha="center", va="center",
                color="white" if cm[i, j] > cm.max()/2 else "black", fontweight='bold')
ax6.set_title('Confusion Matrix')
plt.colorbar(im, ax=ax6)

# 7-9. å…¶ä»–åœ–è¡¨ï¼ˆç°¡åŒ–ï¼‰
ax7 = plt.subplot(3, 3, 7)
ax7.text(0.5, 0.5, f'Detection Rate\n{best_results["detection_rate"]:.2%}',
         ha='center', va='center', fontsize=20, fontweight='bold')
ax7.axis('off')

ax8 = plt.subplot(3, 3, 8)
ax8.text(0.5, 0.5, f'Precision\n{best_results["precision"]:.4f}',
         ha='center', va='center', fontsize=20, fontweight='bold')
ax8.axis('off')

ax9 = plt.subplot(3, 3, 9)
ax9.text(0.5, 0.5, f'F1-Score\n{best_f1:.4f}',
         ha='center', va='center', fontsize=20, fontweight='bold')
ax9.axis('off')

plt.tight_layout()
plt.savefig('autoencoder_final_analysis.png', dpi=150, bbox_inches='tight')
print("ğŸ“Š å·²ä¿å­˜åˆ†æåœ–: autoencoder_final_analysis.png")
plt.show()

print("\n" + "=" * 50)
print("âœ… è¨“ç·´å®Œæˆï¼")
print("=" * 50)
print(f"  - é›¢ç¾¤é»ç§»é™¤: {outliers_removed} ({outlier_ratio:.2%})")
print(f"  - æœ€çµ‚è¨“ç·´æ¨£æœ¬: {len(X_train)}")
print(f"  - æœ€ä½³é–€æª»: {best_threshold:.6f}")
print(f"  - F1-Score: {best_f1:.4f}")
print("=" * 50)