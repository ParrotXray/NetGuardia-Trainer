import joblib
import pandas as pd
from sklearn.preprocessing import StandardScaler
import numpy as np
from sklearn.ensemble import IsolationForest
from pathlib import Path
import os

print("=" * 60)
print("CIC-IDS2017 è³‡æ–™é›†é è™•ç†æµç¨‹")
print("=" * 60)

# ============================================================
# Step 1: è¼‰å…¥æ‰€æœ‰è³‡æ–™é›†
# ============================================================
print("\nğŸ“‚ Step 1: è¼‰å…¥è³‡æ–™é›†...")

file_paths = [
    './csv/Monday-WorkingHours.pcap_ISCX.csv',
    './csv/Tuesday-WorkingHours.pcap_ISCX.csv',
    './csv/Wednesday-workingHours.pcap_ISCX.csv',
    './csv/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv',
    './csv/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv',
    './csv/Friday-WorkingHours-Morning.pcap_ISCX.csv',
    './csv/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv',
    './csv/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv',
    './csv/FTP-BruteForce.csv'
]

datasets = []
for filename in os.listdir("../csv"):
    try:
        print(f"  è¼‰å…¥: {filename}")
        df = pd.read_csv(f"./csv/{filename}", encoding='utf-8', encoding_errors='replace')
        df.columns = df.columns.str.strip()  # æ¸…ç†æ¬„ä½åç¨±
        datasets.append(df)
        print(f"       âœ“ å½¢ç‹€: {df.shape}, æ¨™ç±¤: {df['Label'].nunique()} é¡")
    except FileNotFoundError:
        print(f"       âœ— æª”æ¡ˆä¸å­˜åœ¨: {filename}")
    except Exception as e:
        print(f"       âœ— éŒ¯èª¤: {e}")

if not datasets:
    raise ValueError("âŒ æ²’æœ‰æˆåŠŸè¼‰å…¥ä»»ä½•è³‡æ–™é›†!")

# ============================================================
# Step 2: åˆä½µè³‡æ–™é›†
# ============================================================
print("\nğŸ”— Step 2: åˆä½µè³‡æ–™é›†...")
df_combined = pd.concat(datasets, ignore_index=True)

# ä¿ç•™æ¨™ç±¤
labels = df_combined['Label'].str.replace('ï¿½', '-', regex=False).copy()

print(f"âœ… åˆä½µå¾Œè³‡æ–™: {df_combined.shape}")
print(f"\nğŸ“Š æ¨™ç±¤åˆ†å¸ƒ:")
print(labels.value_counts())

# ============================================================
# Step 3: ç‰¹å¾µæº–å‚™
# ============================================================
print("\nğŸ› ï¸  Step 3: ç‰¹å¾µæº–å‚™...")

# ç§»é™¤éç‰¹å¾µæ¬„ä½
non_feature_cols = ['Flow ID', 'Source IP', 'Destination IP', 'Timestamp', 'Label']
df_features = df_combined.drop(columns=non_feature_cols, errors='ignore')

# æå–æ•¸å€¼ç‰¹å¾µ
X = df_features.select_dtypes(include=[np.number])
print(f"  åŸå§‹ç‰¹å¾µç¶­åº¦: {X.shape}")

# è™•ç†ç•°å¸¸å€¼
X = X.replace([np.inf, -np.inf], np.nan)
X = X.fillna(0)
X = np.clip(X, -1e9, 1e9)

print(f"  æ¸…ç†å¾Œç‰¹å¾µç¶­åº¦: {X.shape}")

# ============================================================
# Step 4: IsolationForest ç•°å¸¸åµæ¸¬ (å¯é¸)
# ============================================================
print("\nğŸ” Step 4: IsolationForest ç•°å¸¸åµæ¸¬...")

contamination_rate = 0.05  # é æœŸç•°å¸¸æ¯”ä¾‹
clf = IsolationForest(
    contamination=contamination_rate,
    random_state=42,
    n_jobs=-1,
    verbose=1
)

print(f"  è¨“ç·´ IsolationForest (contamination={contamination_rate})...")
clf.fit(X)

# é æ¸¬: -1 ç‚ºç•°å¸¸, 1 ç‚ºæ­£å¸¸
predictions = clf.predict(X)
anomaly_if = np.where(predictions == 1, 0, 1)  # è½‰æ›ç‚º 0=æ­£å¸¸, 1=ç•°å¸¸

anomaly_count = anomaly_if.sum()
anomaly_ratio = anomaly_count / len(df_combined) * 100

print(f"  âœ… åµæ¸¬å®Œæˆ!")
print(f"  ğŸš¨ ç•°å¸¸æ•¸é‡: {anomaly_count:,} / {len(df_combined):,} ({anomaly_ratio:.2f}%)")

# ============================================================
# Step 5: è¼¸å‡ºçµæœ
# ============================================================
print("\nğŸ’¾ Step 5: å„²å­˜è™•ç†å¾Œè³‡æ–™...")

# çµ„åˆçµæœ
output = X.copy()
output['anomaly_if'] = anomaly_if
output['Label'] = labels.values

# å„²å­˜ä¸»è¦è¼¸å‡ºæª”æ¡ˆ
output_path = "../output_anomaly.csv"
output.to_csv(output_path, index=False)
print(f"  âœ… å·²å„²å­˜: {output_path}")

# é¡å¤–å„²å­˜çµ±è¨ˆè³‡è¨Š
stats = {
    'total_samples': len(df_combined),
    'total_features': X.shape[1],
    'anomaly_if_count': int(anomaly_count),
    'anomaly_if_ratio': float(anomaly_ratio),
    'label_distribution': labels.value_counts().to_dict()
}

import json
with open('../preprocessing_stats.json', 'w', encoding='utf-8') as f:
    json.dump(stats, f, indent=2, ensure_ascii=False)
print(f"  âœ… å·²å„²å­˜çµ±è¨ˆ: preprocessing_stats.json")

# é¸æ“‡æ€§å„²å­˜æ¨¡å‹
model_path = "../isolation_forest_model.joblib"
joblib.dump(clf, model_path)
print(f"  âœ… å·²å„²å­˜æ¨¡å‹: {model_path}")

print("\n" + "=" * 60)
print("âœ¨ é è™•ç†å®Œæˆ!")
print("=" * 60)
print("\nğŸ“Œ ä¸‹ä¸€æ­¥:")
print("  1. ä½¿ç”¨ 'output_anomaly.csv' è¨“ç·´ Autoencoder")
print("  2. 'anomaly_if' æ¬„ä½ç‚º IsolationForest çš„åƒè€ƒæ¨™è¨˜")
print("  3. 'Label' æ¬„ä½ç‚ºçœŸå¯¦æ¨™ç±¤,å¯ç”¨æ–¼è©•ä¼°")