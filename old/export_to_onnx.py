"""
NetGuardia æ¨¡å‹å°å‡ºå·¥å…·
- å°‡æ‰€æœ‰ Keras/sklearn æ¨¡å‹è½‰ç‚º ONNX
- å°‡æ‰€æœ‰é è™•ç†åƒæ•¸å°å‡ºç‚º JSON
- ä¾› Rust ç¨‹å¼è¼‰å…¥ä½¿ç”¨
"""
import json
import numpy as np
import pandas as pd
import tensorflow as tf
import tf2onnx
import onnx
import joblib
from skl2onnx import convert_sklearn
from skl2onnx.common.data_types import FloatTensorType

print("=" * 60)
print("ğŸš€ NetGuardia æ¨¡å‹å°å‡ºå·¥å…·")
print("=" * 60)

# ============================================================
# 1ï¸âƒ£ è¼‰å…¥æ‰€æœ‰æ¨¡å‹å’Œé…ç½®
# ============================================================
print("\nğŸ“¦ è¼‰å…¥æ¨¡å‹å’Œé…ç½®...")

try:
    deep_ae = tf.keras.models.load_model("../deep_autoencoder.keras")
    print("âœ… Deep Autoencoder è¼‰å…¥")
except Exception as e:
    print(f"âŒ ç„¡æ³•è¼‰å…¥ Deep Autoencoder: {e}")
    exit(1)

try:
    rf = joblib.load("../random_forest.pkl")
    print("âœ… Random Forest è¼‰å…¥")
except Exception as e:
    print(f"âŒ ç„¡æ³•è¼‰å…¥ Random Forest: {e}")
    exit(1)

try:
    mlp = tf.keras.models.load_model("../mlp_improved.keras")
    le = joblib.load("../label_encoder_improved.pkl")
    mlp_name = "mlp_improved"
    print("âœ… MLP Improved è¼‰å…¥")
except:
    try:
        mlp = tf.keras.models.load_model("mlp_attack_classifier.keras")
        le = joblib.load("label_encoder.pkl")
        mlp_name = "mlp_classifier"
        print("âœ… MLP Classifier è¼‰å…¥")
    except Exception as e:
        print(f"âŒ ç„¡æ³•è¼‰å…¥ MLP: {e}")
        exit(1)

try:
    config = joblib.load("../deep_ae_ensemble_config.pkl")
    scaler = config['scaler']
    clip_params = config['clip_params']
    best_strategy = config['best']
    ae_normalization = config.get('ae_normalization', None)  # ğŸ”¥ è¼‰å…¥ AE æ­£è¦åŒ–åƒæ•¸
    print("âœ… Ensemble é…ç½®è¼‰å…¥")

    if ae_normalization:
        print(f"   âœ… AE æ­£è¦åŒ–åƒæ•¸å·²æ‰¾åˆ°:")
        print(f"      Min: {ae_normalization['min']:.6f}")
        print(f"      Max: {ae_normalization['max']:.6f}")
        print(f"      Mean: {ae_normalization['mean']:.6f}")
        print(f"      Std: {ae_normalization['std']:.6f}")
    else:
        print("   âš ï¸  è­¦å‘Šï¼šé…ç½®ä¸­ç¼ºå°‘ ae_normalizationï¼Œè«‹é‡æ–°åŸ·è¡Œ ensemble.py")

except Exception as e:
    print(f"âŒ ç„¡æ³•è¼‰å…¥é…ç½®: {e}")
    exit(1)

# ============================================================
# 2ï¸âƒ£ è½‰æ› Deep Autoencoder ç‚º ONNX
# ============================================================
print("\n" + "=" * 60)
print("ğŸ”„ è½‰æ› Deep Autoencoder â†’ ONNX")
print("=" * 60)

input_dim = deep_ae.input.shape[1]
spec = (tf.TensorSpec((None, input_dim), tf.float32, name="input"),)

model_proto, _ = tf2onnx.convert.from_keras(
    deep_ae,
    input_signature=spec,
    opset=13
)

onnx.save(model_proto, "../deep_autoencoder.onnx")
print("âœ… å·²å„²å­˜: deep_autoencoder.onnx")

# é©—è­‰
onnx_model = onnx.load("../deep_autoencoder.onnx")
onnx.checker.check_model(onnx_model)
print("âœ… ONNX æ¨¡å‹é©—è­‰é€šé")

# ============================================================
# 3ï¸âƒ£ è½‰æ› Random Forest ç‚º ONNX
# ============================================================
print("\n" + "=" * 60)
print("ğŸ”„ è½‰æ› Random Forest â†’ ONNX")
print("=" * 60)

n_features = len(clip_params)
initial_type = [('float_input', FloatTensorType([None, n_features]))]

onx = convert_sklearn(
    rf,
    initial_types=initial_type,
    target_opset=13
)

with open("../random_forest.onnx", "wb") as f:
    f.write(onx.SerializeToString())

print("âœ… å·²å„²å­˜: random_forest.onnx")

# é©—è­‰
onnx_model = onnx.load("../random_forest.onnx")
onnx.checker.check_model(onnx_model)
print("âœ… ONNX æ¨¡å‹é©—è­‰é€šé")

# ============================================================
# 4ï¸âƒ£ è½‰æ› MLP Classifier ç‚º ONNX
# ============================================================
print("\n" + "=" * 60)
print("ğŸ”„ è½‰æ› MLP Classifier â†’ ONNX")
print("=" * 60)

mlp_input_dim = mlp.input_shape[1]
spec = (tf.TensorSpec((None, mlp_input_dim), tf.float32, name="input"),)

model_proto, _ = tf2onnx.convert.from_keras(
    mlp,
    input_signature=spec,
    opset=13
)

mlp_onnx_filename = f"{mlp_name}.onnx"
onnx.save(model_proto, mlp_onnx_filename)
print(f"âœ… å·²å„²å­˜: {mlp_onnx_filename}")

# é©—è­‰
onnx_model = onnx.load(mlp_onnx_filename)
onnx.checker.check_model(onnx_model)
print("âœ… ONNX æ¨¡å‹é©—è­‰é€šé")

# ============================================================
# 5ï¸âƒ£ å°å‡ºé è™•ç†åƒæ•¸ç‚º JSON
# ============================================================
print("\n" + "=" * 60)
print("ğŸ“ å°å‡ºé è™•ç†åƒæ•¸ â†’ JSON")
print("=" * 60)

# æ•´ç† Scaler åƒæ•¸
scaler_params = {
    "mean": scaler.mean_.tolist(),
    "std": scaler.scale_.tolist(),
    "feature_names": scaler.feature_names_in_.tolist() if hasattr(scaler, 'feature_names_in_') else []
}

# æ•´ç† Clip åƒæ•¸
clip_params_json = {}
for col, params in clip_params.items():
    clip_params_json[col] = {
        "lower": float(params['lower']),
        "upper": float(params['upper'])
    }

# æ•´ç† Ensemble åƒæ•¸
ensemble_params = {
    "strategy_name": best_strategy['name'],
    "threshold": float(best_strategy['threshold']),
    "tpr": float(best_strategy['tpr']),
    "fpr": float(best_strategy['fpr']),
    "precision": float(best_strategy['precision']),  # ğŸ”¥ ä¿®æ­£é€™è£¡
    "f1": float(best_strategy['f1'])
}

# ğŸ”¥ æ•´ç† AE æ­£è¦åŒ–åƒæ•¸
if ae_normalization:
    ae_normalization_json = {
        "min": float(ae_normalization['min']),
        "max": float(ae_normalization['max']),
        "mean": float(ae_normalization['mean']),
        "std": float(ae_normalization['std']),
        "median": float(ae_normalization.get('median', 0.0)),
        "p90": float(ae_normalization.get('p90', 0.0)),
        "p95": float(ae_normalization.get('p95', 0.0)),
        "p99": float(ae_normalization.get('p99', 0.0))
    }
else:
    print("âš ï¸  ä½¿ç”¨é è¨­çš„ AE æ­£è¦åŒ–åƒæ•¸ï¼ˆä¸å»ºè­°ï¼‰")
    ae_normalization_json = {
        "min": 0.0,
        "max": 1.0,
        "mean": 0.0,
        "std": 1.0,
        "median": 0.0,
        "p90": 0.0,
        "p95": 0.0,
        "p99": 0.0
    }

# æ•´ç†æ”»æ“Šé¡å‹æ˜ å°„
attack_labels = {
    str(i): label for i, label in enumerate(le.classes_)
}

# çµ„åˆæ‰€æœ‰é…ç½®
config_json = {
    "version": "1.0.0",
    "created_at": pd.Timestamp.now().isoformat(),

    "models": {
        "deep_autoencoder": {
            "file": "deep_autoencoder.onnx",
            "input_dim": int(input_dim),
            "encoding_dim": int(config.get('encoding_dim', 16))
        },
        "random_forest": {
            "file": "random_forest.onnx",
            "n_estimators": int(rf.n_estimators),
            "n_features": int(n_features)
        },
        "mlp_classifier": {
            "file": mlp_onnx_filename,
            "input_dim": int(mlp_input_dim),
            "n_classes": int(len(le.classes_))
        }
    },

    "preprocessing": {
        "clip_params": clip_params_json,
        "scaler": scaler_params,
        "post_scaling_clip": {
            "min": -5.0,
            "max": 5.0
        }
    },

    "ensemble": ensemble_params,

    "ae_normalization": ae_normalization_json,  # ğŸ”¥ ä½¿ç”¨å¯¦éš›åƒæ•¸

    "attack_labels": attack_labels,

    "feature_order": scaler_params["feature_names"]
}

# å„²å­˜ç‚º JSON
with open("../netguardia_config.json", "w", encoding='utf-8') as f:
    json.dump(config_json, f, indent=2, ensure_ascii=False)

print("âœ… å·²å„²å­˜: netguardia_config.json")

# åŒæ™‚å„²å­˜ä¸€å€‹ç²¾ç°¡ç‰ˆï¼ˆåªåŒ…å«æ¨è«–æ‰€éœ€åƒæ•¸ï¼‰
inference_config = {
    "threshold": ensemble_params["threshold"],
    "strategy_name": ensemble_params["strategy_name"],
    "clip_params": clip_params_json,
    "scaler_mean": scaler_params["mean"],
    "scaler_std": scaler_params["std"],
    "post_clip_min": -5.0,
    "post_clip_max": 5.0,
    "ae_normalization": ae_normalization_json,  # ğŸ”¥ åŠ å…¥æ­£è¦åŒ–åƒæ•¸
    "attack_labels": attack_labels,
    "feature_names": scaler_params["feature_names"]
}

with open("../netguardia_inference.json", "w", encoding='utf-8') as f:
    json.dump(inference_config, f, indent=2, ensure_ascii=False)

print("âœ… å·²å„²å­˜: netguardia_inference.json (ç²¾ç°¡ç‰ˆ)")

# ============================================================
# 6ï¸âƒ£ é©—è­‰å°å‡ºçš„æ¨¡å‹
# ============================================================
print("\n" + "=" * 60)
print("ğŸ§ª é©—è­‰ ONNX æ¨¡å‹")
print("=" * 60)

import onnxruntime as ort

# ç”Ÿæˆæ¸¬è©¦è³‡æ–™
test_input = np.random.randn(1, n_features).astype(np.float32)

# é è™•ç†
for i, col in enumerate(scaler_params["feature_names"]):
    if col in clip_params_json:
        test_input[0, i] = np.clip(
            test_input[0, i],
            clip_params_json[col]['lower'],
            clip_params_json[col]['upper']
        )

# æ¨™æº–åŒ–
test_input_scaled = (test_input - np.array(scaler_params["mean"])) / np.array(scaler_params["std"])
test_input_scaled = np.clip(test_input_scaled, -5, 5).astype(np.float32)

# æ¸¬è©¦ Deep AE
print("\n1. æ¸¬è©¦ Deep Autoencoder...")
session_ae = ort.InferenceSession("../deep_autoencoder.onnx")
ae_output = session_ae.run(None, {"input": test_input_scaled})[0]
ae_mse = np.mean((test_input_scaled - ae_output) ** 2)
print(f"   âœ… AE MSE: {ae_mse:.6f}")

# æ¸¬è©¦ RF
print("\n2. æ¸¬è©¦ Random Forest...")
session_rf = ort.InferenceSession("../random_forest.onnx")
rf_output = session_rf.run(None, {"float_input": test_input_scaled})
rf_proba = rf_output[1][0][1]  # probabilities, attack class
print(f"   âœ… RF Attack Probability: {rf_proba:.6f}")

# æ¸¬è©¦ MLP
print("\n3. æ¸¬è©¦ MLP Classifier...")
session_mlp = ort.InferenceSession(mlp_onnx_filename)
mlp_output = session_mlp.run(None, {"input": test_input_scaled})[0]
predicted_class = np.argmax(mlp_output[0])
confidence = mlp_output[0][predicted_class]
print(f"   âœ… Predicted Class: {predicted_class} ({le.classes_[predicted_class]})")
print(f"   âœ… Confidence: {confidence:.6f}")

# æ¸¬è©¦ Ensembleï¼ˆğŸ”¥ ä½¿ç”¨æ­£ç¢ºçš„æ­£è¦åŒ–ï¼‰
print("\n4. æ¸¬è©¦ Ensemble...")
ae_score_norm = (ae_mse - ae_normalization_json['min']) / \
                (ae_normalization_json['max'] - ae_normalization_json['min'] + 1e-10)
ae_score_norm = np.clip(ae_score_norm, 0, 1)  # è£å‰ªåˆ° [0, 1]
rf_score_norm = rf_proba

print(f"   AE Score (normalized): {ae_score_norm:.6f}")
print(f"   RF Score: {rf_score_norm:.6f}")

# æ ¹æ“šç­–ç•¥è¨ˆç®— ensemble score
if ensemble_params["strategy_name"] == "W_7:3":
    ensemble_score = 0.7 * rf_score_norm + 0.3 * ae_score_norm
elif ensemble_params["strategy_name"] == "W_5:5":
    ensemble_score = 0.5 * rf_score_norm + 0.5 * ae_score_norm
elif ensemble_params["strategy_name"] == "W_3:7":
    ensemble_score = 0.3 * rf_score_norm + 0.7 * ae_score_norm
else:
    ensemble_score = (rf_score_norm + ae_score_norm) / 2.0

is_anomaly = ensemble_score > ensemble_params["threshold"]

print(f"   âœ… Ensemble Score: {ensemble_score:.6f}")
print(f"   âœ… Threshold: {ensemble_params['threshold']:.6f}")
print(f"   âœ… Is Anomaly: {is_anomaly}")

if is_anomaly:
    print(f"   ğŸš¨ é æ¸¬ç‚ºæ”»æ“Š: {le.classes_[predicted_class]} (ä¿¡å¿ƒåº¦: {confidence:.2%})")
else:
    print(f"   âœ… é æ¸¬ç‚ºæ­£å¸¸æµé‡")

# ============================================================
# 7ï¸âƒ£ è¼¸å‡ºæ‘˜è¦
# ============================================================
print("\n" + "=" * 60)
print("âœ… å°å‡ºå®Œæˆï¼")
print("=" * 60)

print("\nğŸ“¦ å°å‡ºçš„æª”æ¡ˆ:")
print("  ONNX æ¨¡å‹:")
print("    - deep_autoencoder.onnx")
print("    - random_forest.onnx")
print(f"    - {mlp_onnx_filename}")
print("\n  JSON é…ç½®:")
print("    - netguardia_config.json (å®Œæ•´é…ç½®)")
print("    - netguardia_inference.json (æ¨è«–å°ˆç”¨)")

print("\nğŸ“Š æ¨¡å‹è³‡è¨Š:")
print(f"  Deep AE: {input_dim} ç¶­ â†’ {config.get('encoding_dim', 16)} ç¶­ bottleneck")
print(f"  Random Forest: {rf.n_estimators} æ£µæ¨¹")
print(f"  MLP: {len(le.classes_)} å€‹æ”»æ“Šé¡åˆ¥")
print(f"  Ensemble: {ensemble_params['strategy_name']} (threshold={ensemble_params['threshold']:.4f})")

print("\nğŸ¯ æ€§èƒ½æŒ‡æ¨™:")
print(f"  TPR: {ensemble_params['tpr']:.2%}")
print(f"  FPR: {ensemble_params['fpr']:.2%}")
print(f"  Precision: {ensemble_params['precision']:.3f}")
print(f"  F1-Score: {ensemble_params['f1']:.3f}")

print("\nğŸ”¥ AE æ­£è¦åŒ–åƒæ•¸:")
print(f"  Min: {ae_normalization_json['min']:.6f}")
print(f"  Max: {ae_normalization_json['max']:.6f}")
print(f"  Mean: {ae_normalization_json['mean']:.6f}")
print(f"  Std: {ae_normalization_json['std']:.6f}")

print("\nğŸš€ ä¸‹ä¸€æ­¥:")
print("  1. å°‡ ONNX æ¨¡å‹å’Œ JSON é…ç½®è¤‡è£½åˆ° Rust å°ˆæ¡ˆ")
print("  2. ä½¿ç”¨ 'ort' crate è¼‰å…¥ ONNX æ¨¡å‹")
print("  3. ä½¿ç”¨ 'serde_json' è¼‰å…¥ JSON é…ç½®")
print("  4. ç¢ºä¿ Rust ä½¿ç”¨ç›¸åŒçš„ AE æ­£è¦åŒ–åƒæ•¸")

print("=" * 60)