"""
Deep Autoencoder + Ensemble for Network Intrusion Detection
ç­–ç•¥:
1. Deep Autoencoder (6 å±¤) - æ›´æ·±çš„ç‰¹å¾µå­¸ç¿’
2. Random Forest - åŸºæ–¼çµ±è¨ˆç‰¹å¾µçš„åˆ†é¡
3. Ensemble - çµåˆå…©è€…çš„å„ªå‹¢
4. ğŸ”¥ è¨˜éŒ„ AE æ­£è¦åŒ–åƒæ•¸ä¾›æ¨è«–ä½¿ç”¨
"""
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from tensorflow import keras
from tensorflow.keras import layers, models, regularizers
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.optimizers import Adam
import tensorflow as tf
import joblib
import matplotlib.pyplot as plt

print("=" * 60)
print("ğŸ¤– Deep Autoencoder + Ensemble")
print("=" * 60)

print(f"TensorFlow: {tf.__version__}")
print(f"GPU: {tf.config.list_physical_devices('GPU')}")

# === 1ï¸âƒ£ è¼‰å…¥è³‡æ–™ ===
print("\nğŸ“‚ è¼‰å…¥è³‡æ–™...")
df = pd.read_csv("../output_anomaly.csv")
df.columns = df.columns.str.strip()
labels = df['Label'].copy()

print(f"âœ… ç¸½æ¨£æœ¬: {len(df):,}")
print(f"   BENIGN: {(labels == 'BENIGN').sum():,}")
print(f"   Attack: {(labels != 'BENIGN').sum():,}")

# === 2ï¸âƒ£ æº–å‚™è³‡æ–™ ===
print("\nğŸ¯ æº–å‚™è³‡æ–™...")

exclude_cols = ['Label', 'anomaly_if']
X_all = df.drop(columns=exclude_cols, errors='ignore')
X_all = X_all.select_dtypes(include=[np.number])

# æ¨™ç±¤: 0=BENIGN, 1=Attack
y_all = (labels != 'BENIGN').astype(int)

# åˆ†å‰²è¨“ç·´é›† (åªç”¨ BENIGN) å’Œæ¸¬è©¦é›†
X_benign = X_all[y_all == 0].copy()
X_test_all = X_all.copy()
y_test = y_all.copy()

print(f"âœ… BENIGN è¨“ç·´: {len(X_benign):,}")
print(f"âœ… å…¨éƒ¨æ¸¬è©¦: {len(X_test_all):,}")
print(f"ğŸ”¢ ç‰¹å¾µæ•¸: {X_all.shape[1]}")

# === 3ï¸âƒ£ é è™•ç† ===
print("\nğŸ§¹ é è™•ç†...")

# æ¸…ç†
X_benign = X_benign.replace([np.inf, -np.inf], np.nan).fillna(0)
X_test_all = X_test_all.replace([np.inf, -np.inf], np.nan).fillna(0)

# Winsorization
clip_params = {}
for col in X_benign.columns:
    lower = X_benign[col].quantile(0.005)
    upper = X_benign[col].quantile(0.995)
    X_benign[col] = np.clip(X_benign[col], lower, upper)
    X_test_all[col] = np.clip(X_test_all[col], lower, upper)
    clip_params[col] = {'lower': lower, 'upper': upper}

# æ¨™æº–åŒ–
scaler = StandardScaler()
X_benign_scaled = scaler.fit_transform(X_benign)
X_test_scaled = scaler.transform(X_test_all)

# å¾Œè™•ç†è£å‰ª
X_benign_scaled = np.clip(X_benign_scaled, -5, 5)
X_test_scaled = np.clip(X_test_scaled, -5, 5)

print(f"âœ… å®Œæˆ")

# === 4ï¸âƒ£ å»ºç«‹ Deep Autoencoder ===
print("\n" + "=" * 60)
print("ğŸ§© å»ºç«‹ Deep Autoencoder (6 å±¤)")
print("=" * 60)

input_dim = X_benign_scaled.shape[1]
encoding_dim = 16  # æ›´å°çš„ bottleneck

print(f"æ¶æ§‹: {input_dim} â†’ 1024 â†’ 512 â†’ 256 â†’ 128 â†’ 64 â†’ {encoding_dim}")

# Encoder
inputs = layers.Input(shape=(input_dim,))

# Layer 1
x = layers.Dense(1024, activation='relu')(inputs)
x = layers.BatchNormalization()(x)
x = layers.Dropout(0.3)(x)

# Layer 2
x = layers.Dense(512, activation='relu')(x)
x = layers.BatchNormalization()(x)
x = layers.Dropout(0.25)(x)

# Layer 3
x = layers.Dense(256, activation='relu')(x)
x = layers.BatchNormalization()(x)
x = layers.Dropout(0.2)(x)

# Layer 4
x = layers.Dense(128, activation='relu')(x)
x = layers.BatchNormalization()(x)
x = layers.Dropout(0.15)(x)

# Layer 5
x = layers.Dense(64, activation='relu')(x)
x = layers.BatchNormalization()(x)

# Bottleneck (Layer 6) - åŠ å…¥ L2 æ­£å‰‡åŒ–
encoded = layers.Dense(encoding_dim, activation='relu',
                       kernel_regularizer=regularizers.l2(0.0001),
                       name='bottleneck')(x)

# Decoder (å°ç¨±çµæ§‹)
# Layer 1
x = layers.Dense(64, activation='relu')(encoded)
x = layers.BatchNormalization()(x)
x = layers.Dropout(0.15)(x)

# Layer 2
x = layers.Dense(128, activation='relu')(x)
x = layers.BatchNormalization()(x)
x = layers.Dropout(0.2)(x)

# Layer 3
x = layers.Dense(256, activation='relu')(x)
x = layers.BatchNormalization()(x)
x = layers.Dropout(0.25)(x)

# Layer 4
x = layers.Dense(512, activation='relu')(x)
x = layers.BatchNormalization()(x)
x = layers.Dropout(0.3)(x)

# Layer 5
x = layers.Dense(1024, activation='relu')(x)
x = layers.BatchNormalization()(x)

# Output
decoded = layers.Dense(input_dim, activation='linear')(x)

# å»ºç«‹æ¨¡å‹
deep_ae = models.Model(inputs, decoded, name='deep_autoencoder')

# ç·¨è­¯
deep_ae.compile(
    optimizer=Adam(learning_rate=0.001, clipnorm=1.0),
    loss='mse',
    metrics=['mae']
)

print(f"\nğŸ“Š æ¨¡å‹æ¶æ§‹:")
deep_ae.summary()

print(f"\nğŸ¯ ç¸½åƒæ•¸: {deep_ae.count_params():,}")

# === 5ï¸âƒ£ è¨“ç·´ Deep Autoencoder ===
print("\n" + "=" * 60)
print("ğŸš€ è¨“ç·´ Deep Autoencoder")
print("=" * 60)

callbacks = [
    EarlyStopping(
        monitor='val_loss',
        patience=20,
        restore_best_weights=True,
        verbose=1
    ),
    ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=7,
        min_lr=1e-7,
        verbose=1
    )
]

history = deep_ae.fit(
    X_benign_scaled, X_benign_scaled,
    epochs=100,
    batch_size=1024,
    validation_split=0.15,
    callbacks=callbacks,
    verbose=1
)

epochs = len(history.history['loss'])
print(f"\nâœ… å®Œæˆ: {epochs} epochs")
print(f"   Final Train Loss: {history.history['loss'][-1]:.6f}")
print(f"   Final Val Loss: {history.history['val_loss'][-1]:.6f}")

# === ğŸ”¥ 5.5ï¸âƒ£ è¨ˆç®—è¨“ç·´é›†çš„ AE æ­£è¦åŒ–åƒæ•¸ ===
print("\n" + "=" * 60)
print("ğŸ“Š è¨ˆç®— AE æ­£è¦åŒ–åƒæ•¸ (BENIGN è¨“ç·´é›†)")
print("=" * 60)

# å° BENIGN è¨“ç·´é›†é€²è¡Œé æ¸¬
ae_recon_benign = deep_ae.predict(X_benign_scaled, batch_size=2048, verbose=1)
ae_mse_benign = np.mean(np.square(X_benign_scaled - ae_recon_benign), axis=1)

# è¨ˆç®—çµ±è¨ˆé‡
ae_normalization_params = {
    'min': float(ae_mse_benign.min()),
    'max': float(ae_mse_benign.max()),
    'mean': float(ae_mse_benign.mean()),
    'std': float(ae_mse_benign.std()),
    'median': float(np.median(ae_mse_benign)),
    'p90': float(np.percentile(ae_mse_benign, 90)),
    'p95': float(np.percentile(ae_mse_benign, 95)),
    'p99': float(np.percentile(ae_mse_benign, 99))
}

print(f"âœ… AE MSE çµ±è¨ˆ (BENIGN è¨“ç·´é›†):")
print(f"   Min:    {ae_normalization_params['min']:.6f}")
print(f"   Max:    {ae_normalization_params['max']:.6f}")
print(f"   Mean:   {ae_normalization_params['mean']:.6f}")
print(f"   Std:    {ae_normalization_params['std']:.6f}")
print(f"   Median: {ae_normalization_params['median']:.6f}")
print(f"   P95:    {ae_normalization_params['p95']:.6f}")
print(f"   P99:    {ae_normalization_params['p99']:.6f}")

# === 6ï¸âƒ£ Deep AE é æ¸¬ (å…¨éƒ¨æ¸¬è©¦è³‡æ–™) ===
print("\nğŸ” Deep AE ç•°å¸¸åˆ†æ•¸ (å…¨éƒ¨æ¸¬è©¦è³‡æ–™)...")

# è¨ˆç®— MSE
predictions = deep_ae.predict(X_test_scaled, batch_size=2048, verbose=1)
ae_mse = np.mean(np.square(X_test_scaled - predictions), axis=1)

print(f"âœ… Deep AE ç•°å¸¸åˆ†æ•¸è¨ˆç®—å®Œæˆ")

# é¡¯ç¤ºå…¨éƒ¨è³‡æ–™çš„çµ±è¨ˆ
ae_mse_benign_test = ae_mse[y_test == 0]
ae_mse_attack_test = ae_mse[y_test == 1]

print(f"\nAE MSE çµ±è¨ˆ (æ¸¬è©¦é›†):")
print(f"  BENIGN: Mean={ae_mse_benign_test.mean():.6f}, Median={np.median(ae_mse_benign_test):.6f}")
print(f"  Attack: Mean={ae_mse_attack_test.mean():.6f}, Median={np.median(ae_mse_attack_test):.6f}")
print(f"  åˆ†é›¢åº¦: {ae_mse_attack_test.mean() / ae_mse_benign_test.mean():.2f}x")

# === 7ï¸âƒ£ è¨“ç·´ Random Forest ===
print("\n" + "=" * 60)
print("ğŸŒ² è¨“ç·´ Random Forest")
print("=" * 60)

# ç‚º RF æº–å‚™è¨“ç·´è³‡æ–™ (éœ€è¦æœ‰æ¨™ç±¤)
# ä½¿ç”¨éƒ¨åˆ†è³‡æ–™ä¾†è¨“ç·´ RF (å¹³è¡¡æŠ½æ¨£)
print("æº–å‚™ RF è¨“ç·´è³‡æ–™...")

# å– BENIGN å’Œ Attack å„ 50,000 ç­†
benign_indices = np.where(y_all == 0)[0]
attack_indices = np.where(y_all == 1)[0]

n_samples = min(50000, len(attack_indices))
benign_sample = np.random.choice(benign_indices, n_samples, replace=False)
attack_sample = np.random.choice(attack_indices, n_samples, replace=False)

train_indices = np.concatenate([benign_sample, attack_sample])
np.random.shuffle(train_indices)

X_rf_train = X_test_scaled[train_indices]
y_rf_train = y_test[train_indices]

print(f"RF è¨“ç·´è³‡æ–™: {len(X_rf_train):,} (BENIGN: {n_samples:,}, Attack: {n_samples:,})")

# è¨“ç·´ RF
rf = RandomForestClassifier(
    n_estimators=100,
    max_depth=20,
    min_samples_split=10,
    min_samples_leaf=5,
    max_features='sqrt',
    n_jobs=-1,
    random_state=42,
    verbose=1
)

print("è¨“ç·´ Random Forest...")
rf.fit(X_rf_train, y_rf_train)
print("âœ… RF è¨“ç·´å®Œæˆ")

# RF é æ¸¬ (æ©Ÿç‡)
rf_proba = rf.predict_proba(X_test_scaled)[:, 1]  # Attack çš„æ©Ÿç‡

print(f"âœ… RF é æ¸¬å®Œæˆ")

# === 8ï¸âƒ£ Ensemble ç­–ç•¥ ===
print("\n" + "=" * 60)
print("ğŸ”€ Ensemble ç­–ç•¥")
print("=" * 60)

# ğŸ”¥ ä½¿ç”¨è¨“ç·´é›†çš„ min/max æ­£è¦åŒ–
ae_score_norm = (ae_mse - ae_normalization_params['min']) / \
                (ae_normalization_params['max'] - ae_normalization_params['min'] + 1e-10)
ae_score_norm = np.clip(ae_score_norm, 0, 1)  # è£å‰ªåˆ° [0, 1]

rf_score_norm = rf_proba

print(f"AE Score æ­£è¦åŒ–ç¯„åœ: [{ae_score_norm.min():.4f}, {ae_score_norm.max():.4f}]")
print(f"RF Score ç¯„åœ: [{rf_score_norm.min():.4f}, {rf_score_norm.max():.4f}]")

print("\næ¸¬è©¦å¤šç¨® Ensemble ç­–ç•¥...")

strategies = {}

# ç­–ç•¥ 1: ä¸åŒæ¬Šé‡çµ„åˆ
for ae_w in [0.3, 0.4, 0.5, 0.6, 0.7]:
    rf_w = 1 - ae_w
    name = f"W_{int(ae_w*10)}:{int(rf_w*10)}"
    strategies[name] = ae_w * ae_score_norm + rf_w * rf_score_norm

# ç­–ç•¥ 2: Max
strategies['Max'] = np.maximum(ae_score_norm, rf_score_norm)

# ç­–ç•¥ 3: Min
strategies['Min'] = np.minimum(ae_score_norm, rf_score_norm)

# ç­–ç•¥ 4: Product
strategies['Product'] = ae_score_norm * rf_score_norm

# ç­–ç•¥ 5: Average
strategies['Average'] = (ae_score_norm + rf_score_norm) / 2

# === 9ï¸âƒ£ è©•ä¼° ===
print(f"\n{'Strategy':<12} {'Threshold':>10} {'TPR':>7} {'FPR':>7} {'Prec':>7} {'F1':>7}")
print("-" * 60)

results = []

for name, score in strategies.items():
    # å°‹æ‰¾æœ€ä½³é–€æª» (åŸºæ–¼ F1)
    thresholds = np.percentile(score[y_test == 0], [90, 92, 94, 95, 96, 97, 98, 99])

    best_f1 = 0
    best_threshold = None
    best_metrics = None

    for threshold in thresholds:
        pred = (score > threshold).astype(int)

        tp = ((y_test == 1) & (pred == 1)).sum()
        fp = ((y_test == 0) & (pred == 1)).sum()
        fn = ((y_test == 1) & (pred == 0)).sum()
        tn = ((y_test == 0) & (pred == 0)).sum()

        tpr = tp / (tp + fn) if (tp + fn) > 0 else 0
        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0
        prec = tp / (tp + fp) if (tp + fp) > 0 else 0
        f1 = 2 * prec * tpr / (prec + tpr) if (prec + tpr) > 0 else 0

        if f1 > best_f1 and prec > 0.5:  # ç¢ºä¿ precision > 0.5
            best_f1 = f1
            best_threshold = threshold
            best_metrics = {'tp': tp, 'fp': fp, 'fn': fn, 'tn': tn,
                          'tpr': tpr, 'fpr': fpr, 'precision': prec, 'f1': f1}

    if best_metrics:
        print(f"{name:<12} {best_threshold:>10.4f} {best_metrics['tpr']:>6.1%} "
              f"{best_metrics['fpr']:>6.1%} {best_metrics['precision']:>6.2f} "
              f"{best_metrics['f1']:>6.3f}")

        results.append({
            'name': name,
            'score': score,
            'threshold': best_threshold,
            **best_metrics
        })

# æ‰¾æœ€ä½³ç­–ç•¥
best = max(results, key=lambda x: x['f1'])

print(f"\nğŸ† æœ€ä½³ç­–ç•¥: {best['name']}")
print(f"   Threshold: {best['threshold']:.4f}")
print(f"   TPR: {best['tpr']:.2%}")
print(f"   FPR: {best['fpr']:.2%}")
print(f"   Precision: {best['precision']:.3f}")
print(f"   F1: {best['f1']:.3f}")

# === å„æ”»æ“Šé¡å‹ ===
print("\nğŸ¯ å„æ”»æ“Šé¡å‹åµæ¸¬ç‡:")
for at in sorted(labels[labels != 'BENIGN'].unique()):
    mask = (labels == at)
    detected = (best['score'][mask] > best['threshold']).sum()
    total = mask.sum()
    rate = detected / total if total > 0 else 0
    status = 'âœ…' if rate > 0.5 else 'âš ï¸' if rate > 0.2 else 'âŒ'
    print(f"{status} {at[:30]:<30} {detected:>6}/{total:<6} ({rate:>6.1%})")

# === ğŸ”¥ å„²å­˜ï¼ˆåŠ å…¥ AE æ­£è¦åŒ–åƒæ•¸ï¼‰===
print("\nğŸ’¾ å„²å­˜...")

output = X_all.copy()
output['deep_ae_mse'] = ae_mse
output['rf_proba'] = rf_proba
output['ensemble_score'] = best['score']
output['ensemble_anomaly'] = (best['score'] > best['threshold']).astype(int)
output['Label'] = labels.values

output.to_csv("output_deep_ae_ensemble.csv", index=False)
print(f"âœ… output_deep_ae_ensemble.csv")

deep_ae.save("deep_autoencoder.keras")
joblib.dump(rf, "../random_forest.pkl")
joblib.dump({
    'scaler': scaler,
    'clip_params': clip_params,
    'best': best,
    'results': results,
    'encoding_dim': encoding_dim,
    'ae_normalization': ae_normalization_params  # ğŸ”¥ æ–°å¢
}, "../deep_ae_ensemble_config.pkl")
print(f"âœ… deep_autoencoder.keras, random_forest.pkl, deep_ae_ensemble_config.pkl")

print(f"\nğŸ“Š é…ç½®æª”åŒ…å«:")
print(f"   - Scaler åƒæ•¸")
print(f"   - Clip åƒæ•¸ ({len(clip_params)} å€‹ç‰¹å¾µ)")
print(f"   - Ensemble æœ€ä½³ç­–ç•¥: {best['name']}")
print(f"   - ğŸ”¥ AE æ­£è¦åŒ–åƒæ•¸ (Min/Max/Mean/Std/P95/P99)")

# === è¦–è¦ºåŒ– ===
print("\nğŸ“Š ç”Ÿæˆè¦–è¦ºåŒ–...")

fig, axes = plt.subplots(2, 3, figsize=(18, 10))

# 1. è¨“ç·´æ›²ç·š
ax = axes[0, 0]
ax.plot(history.history['loss'], label='Train', linewidth=2)
ax.plot(history.history['val_loss'], label='Val', linewidth=2)
ax.set_xlabel('Epoch')
ax.set_ylabel('Loss')
ax.set_title('Deep AE Training History')
ax.legend()
ax.grid(alpha=0.3)

# 2. åˆ†æ•¸åˆ†ä½ˆ
ax = axes[0, 1]
bins = 50
ax.hist(best['score'][y_test == 0], bins=bins, alpha=0.7, label='BENIGN', color='green', density=True)
ax.hist(best['score'][y_test == 1], bins=bins, alpha=0.7, label='Attack', color='red', density=True)
ax.axvline(best['threshold'], color='black', linestyle='--', linewidth=2, label='Threshold')
ax.set_xlabel('Ensemble Score')
ax.set_title('Score Distribution')
ax.legend()
ax.grid(alpha=0.3)

# 3. ç­–ç•¥æ¯”è¼ƒ
ax = axes[0, 2]
top_strategies = sorted(results, key=lambda x: x['f1'], reverse=True)[:8]
names = [r['name'] for r in top_strategies]
f1s = [r['f1'] for r in top_strategies]
colors = ['gold' if r['name'] == best['name'] else 'steelblue' for r in top_strategies]
ax.barh(names, f1s, color=colors)
ax.set_xlabel('F1-Score')
ax.set_title('Ensemble Strategies')
ax.grid(alpha=0.3, axis='x')

# 4. æ··æ·†çŸ©é™£
ax = axes[1, 0]
cm = np.array([[best['tn'], best['fp']], [best['fn'], best['tp']]])
im = ax.imshow(cm, cmap='Blues')
for i in range(2):
    for j in range(2):
        text = f"{cm[i,j]:,}\n({cm[i,j]/cm.sum():.1%})"
        color = 'white' if cm[i,j] > cm.max()/2 else 'black'
        ax.text(j, i, text, ha='center', va='center', color=color, fontweight='bold', fontsize=10)
ax.set_xticks([0,1])
ax.set_yticks([0,1])
ax.set_xticklabels(['Normal', 'Attack'])
ax.set_yticklabels(['Normal', 'Attack'])
ax.set_title(f'Confusion Matrix ({best["name"]})')

# 5. AE vs RF åˆ†æ•¸æ¯”è¼ƒ
ax = axes[1, 1]
sample_size = min(10000, len(ae_score_norm))
sample_idx = np.random.choice(len(ae_score_norm), sample_size, replace=False)
colors_scatter = ['red' if y_test.iloc[i] == 1 else 'green' for i in sample_idx]
ax.scatter(ae_score_norm[sample_idx], rf_score_norm[sample_idx],
          c=colors_scatter, alpha=0.3, s=1)
ax.plot([0, 1], [0, 1], 'k--', linewidth=1)
ax.set_xlabel('Deep AE Score (normalized)')
ax.set_ylabel('RF Score (probability)')
ax.set_title('AE vs RF Scores')
ax.grid(alpha=0.3)

# 6. ç‰¹å¾µé‡è¦æ€§ (RF)
ax = axes[1, 2]
feature_importance = rf.feature_importances_
top_10_idx = np.argsort(feature_importance)[-10:]
ax.barh(range(10), feature_importance[top_10_idx], color='teal')
ax.set_yticks(range(10))
ax.set_yticklabels([f'F{i}' for i in top_10_idx], fontsize=8)
ax.set_xlabel('Importance')
ax.set_title('Top 10 Feature Importance (RF)')
ax.grid(alpha=0.3, axis='x')

plt.tight_layout()
plt.savefig('deep_ae_ensemble_analysis.png', dpi=150, bbox_inches='tight')
print(f"âœ… deep_ae_ensemble_analysis.png")

print("\n" + "=" * 60)
print("âœ… Deep AE + Ensemble å®Œæˆ!")
print("=" * 60)
print(f"ğŸ¯ Deep AE: 6 layers, Bottleneck={encoding_dim}")
print(f"ğŸŒ² RF: {rf.n_estimators} trees")
print(f"ğŸ† Best: {best['name']}")
print(f"ğŸ“Š TPR: {best['tpr']:.1%}, F1: {best['f1']:.3f}")
print(f"ğŸ”¥ AE æ­£è¦åŒ–: Min={ae_normalization_params['min']:.6f}, Max={ae_normalization_params['max']:.6f}")
print("=" * 60)