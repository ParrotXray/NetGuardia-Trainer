import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.utils.class_weight import compute_class_weight
from imblearn.over_sampling import SMOTE
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Input, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt
import seaborn as sns
import joblib

print("=" * 60)
print("ğŸš€ æ”¹é€²ç‰ˆ MLP (SMOTE + é¡åˆ¥æ¬Šé‡)")
print("=" * 60)

# === 1ï¸âƒ£ è¼‰å…¥è³‡æ–™ ===
print("\nğŸ“‚ è¼‰å…¥è³‡æ–™...")
df = pd.read_csv("output_deep_ae_ensemble.csv")
df.columns = df.columns.str.strip()

config = joblib.load("deep_ae_ensemble_config.pkl")
scaler = config['scaler']
clip_params = config['clip_params']

# åªç”¨ç•°å¸¸æ¨£æœ¬
df_anomaly = df[df['ensemble_anomaly'] == 1].copy()

print(f"ç•°å¸¸æ¨£æœ¬: {len(df_anomaly):,}")

# === 2ï¸âƒ£ æº–å‚™è³‡æ–™ ===
print("\nğŸ”¢ æº–å‚™ç‰¹å¾µ...")

exclude_cols = ['Label', 'deep_ae_mse', 'rf_proba', 'ensemble_score',
                'ensemble_anomaly', 'anomaly_if']
X = df_anomaly.drop(columns=exclude_cols, errors='ignore')
y = df_anomaly['Label']

# æ¸…ç†
X = X.replace([np.inf, -np.inf], np.nan).fillna(0)
for col in X.columns:
    if col in clip_params:
        X[col] = np.clip(X[col], clip_params[col]['lower'], clip_params[col]['upper'])

# æ¨™æº–åŒ–
X_scaled = scaler.transform(X)
X_scaled = np.clip(X_scaled, -5, 5)

# æ¨™ç±¤ç·¨ç¢¼
encoder = LabelEncoder()
y_encoded = encoder.fit_transform(y)

print(f"ç‰¹å¾µç¶­åº¦: {X_scaled.shape}")
print(f"é¡åˆ¥æ•¸: {len(encoder.classes_)}")

# é¡¯ç¤ºåŸå§‹é¡åˆ¥åˆ†å¸ƒ
print(f"\nğŸ“Š åŸå§‹é¡åˆ¥åˆ†å¸ƒ:")
for idx, label in enumerate(encoder.classes_):
    count = (y_encoded == idx).sum()
    print(f"  {idx:2d}. {label:<35} {count:>7,}")

# === 3ï¸âƒ£ åˆ†å‰²è³‡æ–™ ===
print("\nğŸ“Š åˆ†å‰²è³‡æ–™...")
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y_encoded,
    test_size=0.2,
    random_state=42,
    stratify=y_encoded
)

print(f"è¨“ç·´é›†: {X_train.shape[0]:,}")
print(f"æ¸¬è©¦é›†: {X_test.shape[0]:,}")

# === 4ï¸âƒ£ SMOTE è³‡æ–™å¢å¼· ===
print("\n" + "=" * 60)
print("ğŸ”„ SMOTE è³‡æ–™å¢å¼·")
print("=" * 60)

# è¨ˆç®—æ¯å€‹é¡åˆ¥çš„æ¨£æœ¬æ•¸
unique, counts = np.unique(y_train, return_counts=True)
class_counts = dict(zip(unique, counts))

# è¨­å®š SMOTE ç­–ç•¥ï¼šå°‡å°‘æ•¸é¡åˆ¥æå‡åˆ°å¤šæ•¸é¡åˆ¥çš„ 50%
max_count = max(counts)
sampling_strategy = {}
for cls, count in class_counts.items():
    if count < max_count * 0.5:  # å°‘æ–¼å¤šæ•¸é¡åˆ¥ 50%
        sampling_strategy[cls] = int(max_count * 0.5)

print(f"\nSMOTE ç­–ç•¥:")
for cls in sampling_strategy:
    label = encoder.classes_[cls]
    original = class_counts[cls]
    target = sampling_strategy[cls]
    print(f"  {label:<35} {original:>6,} â†’ {target:>6,} (+{target-original:,})")

# æ‡‰ç”¨ SMOTE
print("\nåŸ·è¡Œ SMOTE...")
smote = SMOTE(
    sampling_strategy=sampling_strategy,
    k_neighbors=min(5, min([class_counts[c] for c in sampling_strategy]) - 1),
    random_state=42
)
X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)

print(f"\nâœ… SMOTE å®Œæˆ")
print(f"è¨“ç·´é›†å¤§å°: {X_train.shape[0]:,} â†’ {X_train_balanced.shape[0]:,}")

# é¡¯ç¤ºå¹³è¡¡å¾Œçš„åˆ†å¸ƒ
print(f"\nğŸ“Š å¹³è¡¡å¾Œé¡åˆ¥åˆ†å¸ƒ:")
unique, counts = np.unique(y_train_balanced, return_counts=True)
for cls, count in zip(unique, counts):
    label = encoder.classes_[cls]
    print(f"  {label:<35} {count:>7,}")

# === 5ï¸âƒ£ è¨ˆç®—é¡åˆ¥æ¬Šé‡ ===
print("\n" + "=" * 60)
print("âš–ï¸ è¨ˆç®—é¡åˆ¥æ¬Šé‡")
print("=" * 60)

class_weights = compute_class_weight(
    class_weight='balanced',
    classes=np.unique(y_train_balanced),
    y=y_train_balanced
)
class_weight_dict = dict(enumerate(class_weights))

print(f"\né¡åˆ¥æ¬Šé‡ (å‰10å€‹):")
for idx in range(min(10, len(class_weights))):
    label = encoder.classes_[idx]
    weight = class_weights[idx]
    print(f"  {label:<35} {weight:.4f}")

# === 6ï¸âƒ£ å»ºç«‹æ”¹é€²ç‰ˆ MLP ===
print("\n" + "=" * 60)
print("ğŸ§© å»ºç«‹æ”¹é€²ç‰ˆ MLP")
print("=" * 60)

n_classes = len(encoder.classes_)
input_dim = X_train_balanced.shape[1]

mlp_improved = Sequential([
    Input(shape=(input_dim,)),
    Dense(512, activation='relu'),
    BatchNormalization(),
    Dropout(0.4),
    Dense(256, activation='relu'),
    BatchNormalization(),
    Dropout(0.3),
    Dense(128, activation='relu'),
    BatchNormalization(),
    Dropout(0.2),
    Dense(64, activation='relu'),
    Dropout(0.1),
    Dense(n_classes, activation='softmax')
], name='mlp_improved')

mlp_improved.compile(
    optimizer=Adam(learning_rate=0.001),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

print("\nğŸ“ æ¨¡å‹æ¶æ§‹:")
mlp_improved.summary()

# === 7ï¸âƒ£ è¨“ç·´ ===
print("\n" + "=" * 60)
print("ğŸš€ è¨“ç·´æ”¹é€²ç‰ˆ MLP")
print("=" * 60)

callbacks = [
    EarlyStopping(
        monitor='val_loss',
        patience=15,
        restore_best_weights=True,
        verbose=1
    ),
    ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=7,
        min_lr=1e-7,
        verbose=1
    )
]

history = mlp_improved.fit(
    X_train_balanced, y_train_balanced,
    epochs=100,
    batch_size=512,
    validation_data=(X_test, y_test),
    callbacks=callbacks,
    class_weight=class_weight_dict,  # ğŸ”¥ ä½¿ç”¨é¡åˆ¥æ¬Šé‡
    verbose=1
)

print(f"\nâœ… è¨“ç·´å®Œæˆ")

# === 8ï¸âƒ£ è©•ä¼° ===
print("\n" + "=" * 60)
print("ğŸ“Š è©•ä¼°çµæœ")
print("=" * 60)

loss, acc = mlp_improved.evaluate(X_test, y_test, verbose=0)
print(f"\næ¸¬è©¦é›†æº–ç¢ºç‡: {acc:.4f}")
print(f"æ¸¬è©¦é›†æå¤±: {loss:.4f}")

y_pred = np.argmax(mlp_improved.predict(X_test, verbose=0), axis=1)

print("\nğŸ“‹ è©³ç´°åˆ†é¡å ±å‘Š:")
print(classification_report(y_test, y_pred, target_names=encoder.classes_, digits=4))

# === 9ï¸âƒ£ é‡é»æª¢æŸ¥ XSS ===
print("\n" + "=" * 60)
print("ğŸ¯ XSS åˆ†é¡çµæœ")
print("=" * 60)

xss_idx = list(encoder.classes_).index('Web Attack ï¿½ XSS')
xss_mask_test = (y_test == xss_idx)

if xss_mask_test.sum() > 0:
    xss_correct = (y_pred[xss_mask_test] == xss_idx).sum()
    xss_total = xss_mask_test.sum()
    xss_accuracy = xss_correct / xss_total

    print(f"\nXSS æ€§èƒ½:")
    print(f"  æ¸¬è©¦æ¨£æœ¬: {xss_total}")
    print(f"  æ­£ç¢ºé æ¸¬: {xss_correct}")
    print(f"  æº–ç¢ºç‡: {xss_accuracy:.1%}")

    # XSS è¢«èª¤åˆ¤æˆä»€éº¼
    xss_predictions = y_pred[xss_mask_test]
    print(f"\n  XSS è¢«é æ¸¬ç‚º:")
    unique, counts = np.unique(xss_predictions, return_counts=True)
    for cls, count in zip(unique, counts):
        label = encoder.classes_[cls]
        pct = count / xss_total * 100
        print(f"    {label:<35} {count:>3} ({pct:>5.1f}%)")

# === ğŸ”Ÿ æ¯”è¼ƒæ”¹é€² ===
print("\n" + "=" * 60)
print("ğŸ“ˆ æ”¹é€²å°æ¯”")
print("=" * 60)

print(f"\n{'é¡åˆ¥':<35} {'åŸå§‹':<10} {'æ”¹é€²å¾Œ':<10} {'è®ŠåŒ–'}")
print("-" * 70)

# å˜—è©¦è¼‰å…¥åŸå§‹çµæœ
try:
    df_old = pd.read_csv("output_mlp.csv")
    y_old_true = df_old['Label']
    y_old_pred = df_old['predicted_label']

    for label in encoder.classes_:
        # åŸå§‹æº–ç¢ºç‡
        mask_old = (y_old_true == label)
        if mask_old.sum() > 0:
            old_acc = (y_old_pred[mask_old] == label).sum() / mask_old.sum()
        else:
            old_acc = 0

        # æ–°æº–ç¢ºç‡
        label_idx = list(encoder.classes_).index(label)
        mask_new = (y_test == label_idx)
        if mask_new.sum() > 0:
            new_acc = (y_pred[mask_new] == label_idx).sum() / mask_new.sum()
        else:
            new_acc = 0

        change = new_acc - old_acc
        change_str = f"{change:+.1%}" if change != 0 else "  -"

        print(f"{label:<35} {old_acc:>8.1%}  {new_acc:>8.1%}  {change_str}")

except:
    print("âš ï¸ æ‰¾ä¸åˆ°åŸå§‹çµæœï¼Œç„¡æ³•æ¯”è¼ƒ")

# === 11ï¸âƒ£ å„²å­˜ ===
print("\nğŸ’¾ å„²å­˜æ”¹é€²ç‰ˆæ¨¡å‹...")

mlp_improved.save("mlp_improved.keras")
joblib.dump(encoder, "label_encoder_improved.pkl")

config_improved = {
    'encoder': encoder,
    'scaler': scaler,
    'clip_params': clip_params,
    'class_weights': class_weight_dict,
    'smote_strategy': sampling_strategy,
    'test_accuracy': acc,
    'test_loss': loss
}
joblib.dump(config_improved, "mlp_improved_config.pkl")

print("âœ… å·²ä¿å­˜:")
print("  - mlp_improved.keras")
print("  - label_encoder_improved.pkl")
print("  - mlp_improved_config.pkl")

# === 12ï¸âƒ£ è¦–è¦ºåŒ– ===
print("\nğŸ“Š ç”Ÿæˆè¦–è¦ºåŒ–...")

fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# 1. æ··æ·†çŸ©é™£
ax = axes[0, 0]
cm = confusion_matrix(y_test, y_pred)
cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
sns.heatmap(cm_normalized, annot=False, cmap='Blues', ax=ax,
            xticklabels=encoder.classes_, yticklabels=encoder.classes_)
ax.set_title('Confusion Matrix (Normalized)')
ax.set_xlabel('Predicted')
ax.set_ylabel('True')

# 2. è¨“ç·´æ›²ç·š
ax = axes[0, 1]
ax.plot(history.history['accuracy'], label='Train', linewidth=2)
ax.plot(history.history['val_accuracy'], label='Val', linewidth=2)
ax.set_title('Training History')
ax.set_xlabel('Epoch')
ax.set_ylabel('Accuracy')
ax.legend()
ax.grid(alpha=0.3)

# 3. å„é¡åˆ¥æº–ç¢ºç‡
ax = axes[1, 0]
accuracies = []
labels_list = []
for idx, label in enumerate(encoder.classes_):
    mask = (y_test == idx)
    if mask.sum() > 0:
        acc = (y_pred[mask] == idx).sum() / mask.sum()
        accuracies.append(acc)
        labels_list.append(label[:20])  # æˆªæ–·é•·æ¨™ç±¤

y_pos = np.arange(len(labels_list))
colors = ['red' if acc < 0.5 else 'orange' if acc < 0.8 else 'green'
          for acc in accuracies]
ax.barh(y_pos, accuracies, color=colors)
ax.set_yticks(y_pos)
ax.set_yticklabels(labels_list, fontsize=8)
ax.set_xlabel('Accuracy')
ax.set_title('Per-Class Accuracy')
ax.grid(alpha=0.3, axis='x')

# 4. é¡åˆ¥æ¨£æœ¬åˆ†å¸ƒ
ax = axes[1, 1]
train_dist = np.bincount(y_train_balanced)
test_dist = np.bincount(y_test)
x = np.arange(len(encoder.classes_))
width = 0.35
ax.bar(x - width/2, train_dist, width, label='Train (SMOTE)', alpha=0.8)
ax.bar(x + width/2, test_dist, width, label='Test', alpha=0.8)
ax.set_xlabel('Class')
ax.set_ylabel('Count')
ax.set_title('Class Distribution')
ax.legend()
ax.grid(alpha=0.3, axis='y')

plt.tight_layout()
plt.savefig('mlp_improved_analysis.png', dpi=150, bbox_inches='tight')
print("âœ… å·²ä¿å­˜: mlp_improved_analysis.png")

print("\n" + "=" * 60)
print("âœ… æ”¹é€²ç‰ˆ MLP å®Œæˆï¼")
print("=" * 60)
print(f"ğŸ“Š æ¸¬è©¦æº–ç¢ºç‡: {acc:.4f}")
print(f"ğŸ¯ é‡é»: æª¢æŸ¥ä¸Šæ–¹ XSS çš„æº–ç¢ºç‡æ˜¯å¦æ”¹å–„")
print("=" * 60)