"""
MLP è¨“ç·´éšæ®µï¼šåªç”¨ Autoencoder åµæ¸¬åˆ°çš„ç•°å¸¸æ¨£æœ¬è¨“ç·´åˆ†é¡å™¨
ç”Ÿæˆ mlp_attack_classifier.h5, label_encoder.pkl
"""
import pandas as pd
import numpy as np
from keras import Input
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
import matplotlib.pyplot as plt
import seaborn as sns
import joblib

print("=" * 50)
print("ğŸ§  Step 3: MLP åˆ†é¡å™¨è¨“ç·´")
print("=" * 50)

# === 1ï¸âƒ£ è®€å– Autoencoder è¼¸å‡º ===
df = pd.read_csv("output_autoencoder.csv")
df.columns = df.columns.str.strip()

print(f"âœ… è¼‰å…¥è³‡æ–™: {df.shape}")

# === 2ï¸âƒ£ æª¢æŸ¥æ¬„ä½ ===
if 'is_anomaly' not in df.columns:
    raise KeyError("âŒ æ‰¾ä¸åˆ° is_anomaly æ¬„ä½ï¼Œè«‹å…ˆåŸ·è¡Œ Autoencoder è¨“ç·´ã€‚")

if 'Label' not in df.columns:
    raise KeyError("âŒ æ‰¾ä¸åˆ° Label æ¬„ä½ï¼Œè«‹ç¢ºèªè³‡æ–™å®Œæ•´æ€§ã€‚")

# === 3ï¸âƒ£ åªç”¨ç•°å¸¸æ¨£æœ¬è¨“ç·´ MLP ===
df_anomaly = df[df['is_anomaly'] == 1].copy()
print(f"ğŸš¨ ç•°å¸¸æ¨£æœ¬æ•¸é‡: {len(df_anomaly)} / {len(df)}")
print(f"ğŸ“‹ ç•°å¸¸æ¨£æœ¬æ¨™ç±¤åˆ†å¸ƒ:\n{df_anomaly['Label'].value_counts()}")

# === 4ï¸âƒ£ æº–å‚™ç‰¹å¾µèˆ‡æ¨™ç±¤ ===
# ç§»é™¤æ‰€æœ‰éç‰¹å¾µæ¬„ä½
exclude_cols = ['Label', 'anomaly_score', 'is_anomaly', 'anomaly_if']
X = df_anomaly.drop(columns=exclude_cols, errors='ignore')
y = df_anomaly['Label']

print(f"ğŸ”¢ ç‰¹å¾µç¶­åº¦: {X.shape}")

# === 5ï¸âƒ£ æ¸…ç†æ•¸å€¼ ===
X = X.replace([np.inf, -np.inf], np.nan).fillna(0)
X = np.clip(X, -1e9, 1e9)
print("âœ… è³‡æ–™æ¸…ç†å®Œæˆ")

# === 6ï¸âƒ£ ä½¿ç”¨ Autoencoder çš„ scalerï¼ˆé—œéµï¼ï¼‰===
scaler = joblib.load("scaler_ae.pkl")
X_scaled = scaler.transform(X)
print("âœ… å·²ä½¿ç”¨ Autoencoder çš„ scaler æ¨™æº–åŒ–ç‰¹å¾µ")

# === 7ï¸âƒ£ æ¨™ç±¤ç·¨ç¢¼ ===
encoder = LabelEncoder()
y_encoded = encoder.fit_transform(y)
print(f"ğŸ“¦ Label ç·¨ç¢¼å®Œæˆï¼Œå…± {len(encoder.classes_)} é¡åˆ¥ï¼š")
for idx, label in enumerate(encoder.classes_):
    count = (y_encoded == idx).sum()
    print(f"  {idx}: {label} ({count} samples)")

# === 8ï¸âƒ£ åˆ†å‰²è³‡æ–™é›† ===
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y_encoded, 
    test_size=0.2, 
    random_state=42, 
    stratify=y_encoded
)
print(f"\nğŸ“Š è¨“ç·´é›†: {X_train.shape}, æ¸¬è©¦é›†: {X_test.shape}")

# === 9ï¸âƒ£ å»ºç«‹ MLP æ¨¡å‹ ===
mlp = Sequential([
    Input(shape=(X_train.shape[1],)),
    Dense(256, activation='relu'),
    Dropout(0.3),
    Dense(128, activation='relu'),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dropout(0.2),
    Dense(len(np.unique(y_encoded)), activation='softmax')
])

mlp.compile(
    optimizer='adam', 
    loss='sparse_categorical_crossentropy', 
    metrics=['accuracy']
)

print("\nğŸ“ MLP æ¨¡å‹çµæ§‹:")
mlp.summary()

# === ğŸ”Ÿ è¨“ç·´ MLP ===
print("\nğŸš€ é–‹å§‹è¨“ç·´ MLP...")
history = mlp.fit(
    X_train, y_train, 
    epochs=30, 
    batch_size=256, 
    validation_data=(X_test, y_test), 
    verbose=1
)

# === 11ï¸âƒ£ è©•ä¼°æ¨¡å‹ ===
loss, acc = mlp.evaluate(X_test, y_test, verbose=0)
print(f"\nâœ… æ¸¬è©¦é›†æº–ç¢ºç‡: {acc:.4f}")
print(f"ğŸ“‰ æ¸¬è©¦é›†æå¤±: {loss:.4f}")

# è©³ç´°è©•ä¼°å ±å‘Š
y_pred = np.argmax(mlp.predict(X_test, verbose=0), axis=1)
print("\nğŸ“Š åˆ†é¡å ±å‘Š:")
print(classification_report(y_test, y_pred, target_names=encoder.classes_))

# === 12ï¸âƒ£ æ··æ·†çŸ©é™£ ===
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
            xticklabels=encoder.classes_, 
            yticklabels=encoder.classes_)
plt.title('MLP Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.tight_layout()
plt.savefig('mlp_confusion_matrix.png', dpi=150, bbox_inches='tight')
print("ğŸ“Š å·²ä¿å­˜æ··æ·†çŸ©é™£: mlp_confusion_matrix.png")

# === 13ï¸âƒ£ è¨“ç·´æ›²ç·š ===
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

# Accuracy
ax1.plot(history.history['accuracy'], label='Train Acc')
ax1.plot(history.history['val_accuracy'], label='Val Acc')
ax1.set_title('MLP Accuracy Curve')
ax1.set_xlabel('Epoch')
ax1.set_ylabel('Accuracy')
ax1.legend()
ax1.grid(True, alpha=0.3)

# Loss
ax2.plot(history.history['loss'], label='Train Loss')
ax2.plot(history.history['val_loss'], label='Val Loss')
ax2.set_title('MLP Loss Curve')
ax2.set_xlabel('Epoch')
ax2.set_ylabel('Loss')
ax2.legend()
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('mlp_training_curves.png', dpi=150, bbox_inches='tight')
print("ğŸ“Š å·²ä¿å­˜è¨“ç·´æ›²ç·š: mlp_training_curves.png")

plt.show()

# === 14ï¸âƒ£ é æ¸¬å…¨éƒ¨ç•°å¸¸æ¨£æœ¬ä¸¦è¼¸å‡º ===
pred_all = np.argmax(mlp.predict(X_scaled, verbose=0), axis=1)
df_anomaly["predicted_label"] = encoder.inverse_transform(pred_all)
df_anomaly.to_csv("output_mlp.csv", index=False)
print("\nğŸ’¾ å·²è¼¸å‡ºå«é æ¸¬çµæœçš„æª”æ¡ˆ: output_mlp.csv")

# === 15ï¸âƒ£ å„²å­˜æ¨¡å‹èˆ‡ç·¨ç¢¼å™¨ ===
mlp.save("mlp_attack_classifier.h5")
joblib.dump(encoder, "label_encoder.pkl")
print("âœ… å·²ä¿å­˜æ¨¡å‹: mlp_attack_classifier.h5")
print("âœ… å·²ä¿å­˜ç·¨ç¢¼å™¨: label_encoder.pkl")

print("\n" + "=" * 50)
print("âœ… MLP è¨“ç·´å®Œæˆï¼")
print("=" * 50)