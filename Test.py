"""
æ¸¬è©¦éšæ®µ - Deep AE + Ensemble + MLP æ”¹é€²ç‰ˆç³»çµ±
- è‡ªå‹•è¼‰å…¥ Deep AE + RF Ensemble
- å¤šé–€æª»ç­–ç•¥æ¸¬è©¦
- è©³ç´°çš„æ”»æ“Šé¡å‹åˆ†æ
- æ¼å ±/èª¤å ±æ·±åº¦åˆ†æ
"""
import pandas as pd
import numpy as np
import joblib
from tensorflow.keras.models import load_model
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

print("=" * 60)
print("ğŸ§ª Deep AE + Ensemble ç³»çµ±æ¸¬è©¦")
print("=" * 60)

# === 1ï¸âƒ£ è¼‰å…¥æ¨¡å‹èˆ‡å·¥å…· ===
print("\nğŸ“¦ è¼‰å…¥æ¨¡å‹èˆ‡å·¥å…·...")

try:
    # Deep AE + RF Ensemble
    print("  è¼‰å…¥ Deep Autoencoder...")
    deep_ae = load_model("deep_autoencoder.keras")

    print("  è¼‰å…¥ Random Forest...")
    rf = joblib.load("random_forest.pkl")

    print("  è¼‰å…¥ Ensemble é…ç½®...")
    ensemble_config = joblib.load("deep_ae_ensemble_config.pkl")
    scaler = ensemble_config['scaler']
    clip_params = ensemble_config['clip_params']
    best_strategy = ensemble_config['best']

    print("  è¼‰å…¥ MLP åˆ†é¡å™¨...")
    # å„ªå…ˆä½¿ç”¨æ”¹é€²ç‰ˆï¼Œå¦‚æœä¸å­˜åœ¨å‰‡ä½¿ç”¨åŸå§‹ç‰ˆ
    try:
        mlp = load_model("mlp_improved.keras")
        le = joblib.load("label_encoder_improved.pkl")
        print("    âœ… ä½¿ç”¨æ”¹é€²ç‰ˆ MLP")
    except:
        mlp = load_model("mlp_attack_classifier.keras")
        le = joblib.load("label_encoder.pkl")
        print("    âœ… ä½¿ç”¨åŸå§‹ç‰ˆ MLP")

    print("\nâœ… æ‰€æœ‰æ¨¡å‹è¼‰å…¥æˆåŠŸ")
    print(f"ğŸ“Š Ensemble ç­–ç•¥: {best_strategy['name']}")
    print(f"ğŸ“Š æ¨è–¦é–€æª»: {best_strategy['threshold']:.6f}")
    print(f"ğŸ“Š è¨“ç·´æ™‚æ€§èƒ½: TPR={best_strategy['tpr']:.1%}, F1={best_strategy['f1']:.3f}")

except Exception as e:
    print(f"\nâŒ è¼‰å…¥æ¨¡å‹å¤±æ•—: {e}")
    print("\nè«‹ç¢ºä¿ä»¥ä¸‹æª”æ¡ˆå­˜åœ¨:")
    print("  - deep_autoencoder.keras")
    print("  - random_forest.pkl")
    print("  - deep_ae_ensemble_config.pkl")
    print("  - mlp_improved.keras (æˆ– mlp_attack_classifier.keras)")
    print("  - label_encoder_improved.pkl (æˆ– label_encoder.pkl)")
    exit(1)

# === 2ï¸âƒ£ è®€å–æ¸¬è©¦è³‡æ–™ ===
print("\nğŸ“‚ è®€å–æ¸¬è©¦è³‡æ–™...")

# å˜—è©¦ä¸åŒçš„æ¸¬è©¦æª”æ¡ˆ
test_files = [

    './csv/Friday-WorkingHours-Morning.pcap_ISCX.csv',
    './csv/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv',

]

df = None
for test_file in test_files:
    try:
        df = pd.read_csv(test_file)
        df.columns = df.columns.str.strip()
        print(f"âœ… è¼‰å…¥æ¸¬è©¦è³‡æ–™: {test_file}")
        break
    except:
        continue

if df is None:
    print("âŒ æ‰¾ä¸åˆ°æ¸¬è©¦è³‡æ–™æª”æ¡ˆ")
    exit(1)

print(f"æ¸¬è©¦è³‡æ–™ç¶­åº¦: {df.shape}")

# æª¢æŸ¥æ˜¯å¦æœ‰ Label æ¬„ä½
if 'Label' not in df.columns:
    print("âŒ æ¸¬è©¦è³‡æ–™ç¼ºå°‘ Label æ¬„ä½")
    exit(1)

print(f"\nğŸ“‹ æ¸¬è©¦è³‡æ–™æ¨™ç±¤åˆ†å¸ƒ:")
label_counts = df['Label'].value_counts()
for label, count in label_counts.items():
    print(f"  {label}: {count:,}")

labels = df['Label'].copy()

# === 3ï¸âƒ£ æº–å‚™ç‰¹å¾µ ===
print("\nğŸ”¢ æº–å‚™ç‰¹å¾µ...")

# ç§»é™¤éç‰¹å¾µæ¬„ä½
exclude_cols = ['Flow ID', 'Source IP', 'Destination IP', 'Timestamp', 'Label',
                'deep_ae_mse', 'rf_proba', 'ensemble_score', 'ensemble_anomaly',
                'predicted_label', 'prediction_confidence', 'is_correct', 'anomaly_if']
df_features = df.drop(columns=exclude_cols, errors='ignore')

# é¸æ“‡æ•¸å€¼ç‰¹å¾µ
X = df_features.select_dtypes(include=[np.number])

print(f"åŸå§‹ç‰¹å¾µæ•¸: {X.shape[1]}")

# æ¸…ç†æ•¸æ“š
X = X.replace([np.inf, -np.inf], np.nan).fillna(0)

# === 4ï¸âƒ£ é è™•ç†ï¼ˆWinsorization + æ¨™æº–åŒ–ï¼‰===
print("\nğŸ§¹ é è™•ç†è³‡æ–™...")

# Winsorization (ä½¿ç”¨è¨“ç·´æ™‚çš„è£å‰ªåƒæ•¸)
print("  Winsorization...")
for col in X.columns:
    if col in clip_params:
        X[col] = np.clip(X[col],
                        clip_params[col]['lower'],
                        clip_params[col]['upper'])

# æª¢æŸ¥ç‰¹å¾µå°é½Š
if hasattr(scaler, 'feature_names_in_'):
    scaler_cols = scaler.feature_names_in_
    missing_cols = set(scaler_cols) - set(X.columns)
    if missing_cols:
        print(f"  âš ï¸ æ¸¬è©¦è³‡æ–™ç¼ºå°‘æ¬„ä½: {len(missing_cols)} å€‹")
        for col in missing_cols:
            X[col] = 0
    X = X[scaler_cols]

# æ¨™æº–åŒ–
print("  æ¨™æº–åŒ–...")
X_scaled = scaler.transform(X)
X_scaled = np.clip(X_scaled, -5, 5)

print(f"âœ… é è™•ç†å®Œæˆï¼Œç‰¹å¾µç¶­åº¦: {X_scaled.shape}")

# === 5ï¸âƒ£ é©—è­‰ç¶­åº¦ ===
expected_dim = deep_ae.input_shape[1]
current_dim = X_scaled.shape[1]

if current_dim != expected_dim:
    raise ValueError(f"âŒ ç¶­åº¦ä¸åŒ¹é…ï¼æ¨¡å‹éœ€è¦ {expected_dim} ç¶­ï¼Œè³‡æ–™æœ‰ {current_dim} ç¶­")

print(f"âœ… ç‰¹å¾µç¶­åº¦é©—è­‰é€šé: {current_dim}")

# === 6ï¸âƒ£ Ensemble ç•°å¸¸åµæ¸¬ ===
print("\n" + "=" * 60)
print("ğŸ” åŸ·è¡Œ Ensemble ç•°å¸¸åµæ¸¬")
print("=" * 60)

print("\néšæ®µ 1: Deep Autoencoder é æ¸¬...")
ae_recon = deep_ae.predict(X_scaled, batch_size=2048, verbose=1)
ae_mse = np.mean(np.square(X_scaled - ae_recon), axis=1)

print("\néšæ®µ 2: Random Forest é æ¸¬...")
rf_proba = rf.predict_proba(X_scaled)[:, 1]

print("\néšæ®µ 3: Ensemble Score è¨ˆç®—...")
# æ­£è¦åŒ–
ae_score_norm = (ae_mse - ae_mse.min()) / (ae_mse.max() - ae_mse.min() + 1e-10)
rf_score_norm = rf_proba

# Ensemble (æ ¹æ“šç­–ç•¥)
if best_strategy['name'] == 'W_3:7':
    ensemble_score = 0.3 * ae_score_norm + 0.7 * rf_score_norm
elif best_strategy['name'] == 'W_5:5':
    ensemble_score = 0.5 * ae_score_norm + 0.5 * rf_score_norm
elif best_strategy['name'] == 'W_7:3':
    ensemble_score = 0.7 * ae_score_norm + 0.3 * rf_score_norm
else:
    ensemble_score = (ae_score_norm + rf_score_norm) / 2

print(f"âœ… Ensemble Score è¨ˆç®—å®Œæˆ (ç­–ç•¥: {best_strategy['name']})")

# åˆ†é¡çµ±è¨ˆ
ensemble_benign = ensemble_score[labels == 'BENIGN']
ensemble_attack = ensemble_score[labels != 'BENIGN']

print(f"\nğŸ“Š Ensemble Score çµ±è¨ˆ:")
print(f"  BENIGN:")
print(f"    Mean: {ensemble_benign.mean():.6f}")
print(f"    Median: {np.median(ensemble_benign):.6f}")
print(f"    P95: {np.percentile(ensemble_benign, 95):.6f}")
print(f"    P99: {np.percentile(ensemble_benign, 99):.6f}")
print(f"  Attack:")
print(f"    Mean: {ensemble_attack.mean():.6f}")
print(f"    Median: {np.median(ensemble_attack):.6f}")
print(f"  åˆ†é›¢åº¦: {ensemble_attack.mean() / ensemble_benign.mean():.2f}x")

# === 7ï¸âƒ£ æ¸¬è©¦å¤šå€‹é–€æª»ç­–ç•¥ ===
print("\n" + "=" * 60)
print("ğŸ¯ æ¸¬è©¦å¤šå€‹é–€æª»ç­–ç•¥")
print("=" * 60)

test_thresholds = {
    'Recommended': best_strategy['threshold'],
    'BENIGN_P90': np.percentile(ensemble_benign, 90),
    'BENIGN_P95': np.percentile(ensemble_benign, 95),
    'BENIGN_P97': np.percentile(ensemble_benign, 97),
    'BENIGN_P99': np.percentile(ensemble_benign, 99),
    'Mean+2Std': ensemble_benign.mean() + 2 * ensemble_benign.std(),
    'Mean+3Std': ensemble_benign.mean() + 3 * ensemble_benign.std(),
}

print(f"\n{'ç­–ç•¥':<20} {'é–€æª»å€¼':<12} {'TP':<8} {'FP':<8} {'FN':<8} {'Precision':<10} {'Recall':<10} {'F1':<10}")
print("-" * 100)

threshold_results = []
for name, thresh in sorted(test_thresholds.items(), key=lambda x: x[1]):
    is_ano = (ensemble_score > thresh).astype(int)

    tp = ((labels != 'BENIGN') & (is_ano == 1)).sum()
    fp = ((labels == 'BENIGN') & (is_ano == 1)).sum()
    fn = ((labels != 'BENIGN') & (is_ano == 0)).sum()
    tn = ((labels == 'BENIGN') & (is_ano == 0)).sum()

    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

    print(f"{name:<20} {thresh:<12.6f} {tp:<8} {fp:<8} {fn:<8} {precision:<10.4f} {recall:<10.4f} {f1:<10.4f}")

    threshold_results.append({
        'name': name,
        'threshold': thresh,
        'tp': tp, 'fp': fp, 'fn': fn, 'tn': tn,
        'precision': precision, 'recall': recall, 'f1': f1
    })

# é¸æ“‡æœ€ä½³é–€æª»
best_test_result = max(threshold_results, key=lambda x: x['f1'])
threshold = best_test_result['threshold']
threshold_name = best_test_result['name']

print(f"\nğŸ† æ¸¬è©¦é›†æœ€ä½³é–€æª»: {threshold_name}")
print(f"   é–€æª»å€¼: {threshold:.6f}")
print(f"   F1-Score: {best_test_result['f1']:.4f}")

# === 8ï¸âƒ£ ä½¿ç”¨æœ€ä½³é–€æª»é€²è¡Œé æ¸¬ ===
print("\n" + "=" * 60)
print("ğŸ“Š ç•°å¸¸åµæ¸¬çµæœ")
print("=" * 60)

is_anomaly = (ensemble_score > threshold).astype(int)

print(f"\nåµæ¸¬åˆ°ç•°å¸¸: {is_anomaly.sum():,} / {len(df):,} ({is_anomaly.sum()/len(df):.1%})")

tp = best_test_result['tp']
fp = best_test_result['fp']
fn = best_test_result['fn']
tn = best_test_result['tn']

accuracy = (tp + tn) / (tp + tn + fp + fn)
precision = best_test_result['precision']
recall = best_test_result['recall']
f1 = best_test_result['f1']
fpr = fp / (fp + tn) if (fp + tn) > 0 else 0

print(f"\nğŸ¯ æ€§èƒ½æŒ‡æ¨™:")
print(f"  True Positives (TP):  {tp:>8,}")
print(f"  False Positives (FP): {fp:>8,}")
print(f"  False Negatives (FN): {fn:>8,}")
print(f"  True Negatives (TN):  {tn:>8,}")
print(f"\n  Accuracy:   {accuracy:.4f}")
print(f"  Precision:  {precision:.4f}")
print(f"  Recall/TPR: {recall:.4f}")
print(f"  FPR:        {fpr:.4f}")
print(f"  F1-Score:   {f1:.4f}")

# === 9ï¸âƒ£ å„æ”»æ“Šé¡å‹è©³ç´°åˆ†æ ===
print("\n" + "=" * 60)
print("ğŸ¯ å„æ”»æ“Šé¡å‹åµæ¸¬ç‡")
print("=" * 60)

print(f"\n{'æ”»æ“Šé¡å‹':<35} {'ç¸½æ•¸':<10} {'åµæ¸¬':<10} {'åµæ¸¬ç‡':<10} {'å¹³å‡åˆ†æ•¸':<12}")
print("-" * 80)

attack_analysis = []
for attack_type in sorted(labels[labels != 'BENIGN'].unique()):
    mask_attack = (labels == attack_type)
    total = mask_attack.sum()
    detected = ((labels == attack_type) & (is_anomaly == 1)).sum()
    rate = detected / total if total > 0 else 0

    score_type = ensemble_score[mask_attack]
    mean_score = score_type.mean()

    status = 'âœ…' if rate >= 0.8 else 'âš ï¸' if rate >= 0.5 else 'âŒ'
    print(f"{status} {attack_type:<32} {total:<10,} {detected:<10,} {rate:<9.1%} {mean_score:<12.6f}")

    attack_analysis.append({
        'type': attack_type,
        'total': total,
        'detected': detected,
        'rate': rate,
        'mean_score': mean_score
    })

# æ‰¾å‡ºæœ€é›£åµæ¸¬çš„æ”»æ“Š
if attack_analysis:
    worst_attack = min(attack_analysis, key=lambda x: x['rate'])
    best_attack = max(attack_analysis, key=lambda x: x['rate'])
    print(f"\nâš ï¸ æœ€é›£åµæ¸¬: {worst_attack['type']} ({worst_attack['rate']:.1%})")
    print(f"âœ… æœ€æ˜“åµæ¸¬: {best_attack['type']} ({best_attack['rate']:.1%})")

# === ğŸ”Ÿ MLP åˆ†é¡ç•°å¸¸æ¨£æœ¬ ===
print("\n" + "=" * 60)
print("ğŸ§  MLP æ”»æ“Šåˆ†é¡")
print("=" * 60)

X_anomaly = X_scaled[is_anomaly == 1]

if len(X_anomaly) > 0:
    print(f"\nå° {len(X_anomaly):,} å€‹ç•°å¸¸æ¨£æœ¬é€²è¡Œåˆ†é¡...")

    mlp_expected_dim = mlp.input_shape[1]
    if X_anomaly.shape[1] != mlp_expected_dim:
        raise ValueError(f"âŒ MLP ç¶­åº¦ä¸åŒ¹é…ï¼")

    preds = mlp.predict(X_anomaly, batch_size=2048, verbose=0)
    pred_labels = le.inverse_transform(np.argmax(preds, axis=1))
    pred_confidence = preds.max(axis=1)

    print(f"âœ… åˆ†é¡å®Œæˆ")
    print(f"\nğŸ“‹ é æ¸¬æ”»æ“Šé¡å‹åˆ†å¸ƒ:")
    pred_counts = pd.Series(pred_labels).value_counts()
    for label, count in pred_counts.items():
        pct = count / len(pred_labels) * 100
        print(f"  {label:<35} {count:>6,} ({pct:>5.1f}%)")

    # MLP åˆ†é¡æº–ç¢ºç‡
    true_labels_of_anomalies = labels.values[is_anomaly == 1]
    correct_classification = (true_labels_of_anomalies == pred_labels).sum()
    classification_acc = correct_classification / len(pred_labels)

    print(f"\nğŸ“Š MLP æ•´é«”åˆ†é¡æº–ç¢ºç‡: {classification_acc:.4f} ({correct_classification:,}/{len(pred_labels):,})")

    # åªçœ‹çœŸå¯¦æ”»æ“Šçš„åˆ†é¡æº–ç¢ºç‡
    mask_real_attack = true_labels_of_anomalies != 'BENIGN'
    if mask_real_attack.sum() > 0:
        true_attack_labels = true_labels_of_anomalies[mask_real_attack]
        pred_attack_labels = pred_labels[mask_real_attack]

        attack_correct = (true_attack_labels == pred_attack_labels).sum()
        attack_acc = attack_correct / len(true_attack_labels)

        print(f"ğŸ“Š æ”»æ“Šåˆ†é¡æº–ç¢ºç‡ï¼ˆæ’é™¤ BENIGNï¼‰: {attack_acc:.4f} ({attack_correct:,}/{len(true_attack_labels):,})")

        print(f"\nğŸ“‹ æ”»æ“Šåˆ†é¡è©³ç´°å ±å‘Š:")
        try:
            report = classification_report(true_attack_labels, pred_attack_labels,
                                          zero_division=0, digits=4)
            print(report)
        except:
            print("  (ç„¡æ³•ç”Ÿæˆå ±å‘Š)")
else:
    pred_labels = []
    pred_confidence = []
    print("\nâš ï¸ æ²’æœ‰åµæ¸¬åˆ°ç•°å¸¸æ¨£æœ¬")

# === 11ï¸âƒ£ æ¼å ±åˆ†æ ===
print("\n" + "=" * 60)
print("ğŸš« æ¼å ±ï¼ˆFalse Negativesï¼‰åˆ†æ")
print("=" * 60)

false_negatives_mask = (labels != 'BENIGN') & (is_anomaly == 0)
false_negatives = df[false_negatives_mask]

if len(false_negatives) > 0:
    fn_count = len(false_negatives)
    total_attacks = (labels != 'BENIGN').sum()

    print(f"\næ¼å ±ç¸½æ•¸: {fn_count:,} / {total_attacks:,} ({fn_count/total_attacks:.1%})")

    fn_scores = ensemble_score[false_negatives_mask]
    print(f"\næ¼å ±æ¨£æœ¬çš„ Ensemble Score çµ±è¨ˆ:")
    print(f"  Mean:   {fn_scores.mean():.6f}")
    print(f"  Median: {np.median(fn_scores):.6f}")
    print(f"  Max:    {fn_scores.max():.6f}")
    print(f"  P95:    {np.percentile(fn_scores, 95):.6f}")

    print(f"\nğŸ“‹ æ¼å ±æ”»æ“Šé¡å‹åˆ†å¸ƒ:")
    fn_labels = labels[false_negatives_mask]
    for attack_type in sorted(fn_labels.unique()):
        fn_type_count = (fn_labels == attack_type).sum()
        total_type = (labels == attack_type).sum()
        pct = fn_type_count / total_type * 100
        print(f"  {attack_type:<35} {fn_type_count:>6,} / {total_type:<6,} ({pct:>5.1f}%)")

    # å»ºè­°èª¿æ•´
    fn_score_95 = np.percentile(fn_scores, 95)
    print(f"\nğŸ’¡ è‹¥è¦æ•æ‰ 95% çš„æ¼å ±ï¼Œé–€æª»éœ€é™è‡³: {fn_score_95:.6f}")
    print(f"   (ç•¶å‰é–€æª»: {threshold:.6f})")
else:
    print("\nâœ… æ²’æœ‰æ¼å ±ï¼æ‰€æœ‰æ”»æ“Šéƒ½è¢«åµæ¸¬åˆ°")

# === 12ï¸âƒ£ èª¤å ±åˆ†æ ===
print("\n" + "=" * 60)
print("âš ï¸ èª¤å ±ï¼ˆFalse Positivesï¼‰åˆ†æ")
print("=" * 60)

false_positives_mask = (labels == 'BENIGN') & (is_anomaly == 1)
false_positives = df[false_positives_mask]

if len(false_positives) > 0:
    fp_count = len(false_positives)
    total_benign = (labels == 'BENIGN').sum()

    print(f"\nèª¤å ±ç¸½æ•¸: {fp_count:,} / {total_benign:,} ({fp_count/total_benign:.1%})")

    fp_scores = ensemble_score[false_positives_mask]
    print(f"\nèª¤å ±æ¨£æœ¬çš„ Ensemble Score çµ±è¨ˆ:")
    print(f"  Mean:   {fp_scores.mean():.6f}")
    print(f"  Median: {np.median(fp_scores):.6f}")
    print(f"  Min:    {fp_scores.min():.6f}")
    print(f"  P5:     {np.percentile(fp_scores, 5):.6f}")

    print(f"\nğŸ’¡ è‹¥è¦æ¸›å°‘èª¤å ±åˆ° 1%ï¼Œé–€æª»éœ€æå‡è‡³: {np.percentile(ensemble_benign, 99):.6f}")
    print(f"   (ç•¶å‰é–€æª»: {threshold:.6f})")
else:
    print("\nâœ… æ²’æœ‰èª¤å ±ï¼æ‰€æœ‰æ­£å¸¸æµé‡éƒ½è¢«æ­£ç¢ºè­˜åˆ¥")

# === 13ï¸âƒ£ çµ„åˆè¼¸å‡ºçµæœ ===
print("\nğŸ’¾ å„²å­˜çµæœ...")

output = pd.DataFrame()
output["Label"] = labels.values
output["ae_mse"] = ae_mse
output["rf_proba"] = rf_proba
output["ensemble_score"] = ensemble_score
output["is_anomaly"] = is_anomaly
output["predicted_attack"] = "BENIGN"

if len(pred_labels) > 0:
    output.loc[is_anomaly == 1, "predicted_attack"] = pred_labels
    output.loc[is_anomaly == 1, "confidence"] = pred_confidence

output.to_csv("test_ensemble_results.csv", index=False)
print("âœ… å·²ä¿å­˜: test_ensemble_results.csv")

# === 14ï¸âƒ£ è¦–è¦ºåŒ– ===
print("\nğŸ“Š ç”Ÿæˆè¦–è¦ºåŒ–...")

fig = plt.figure(figsize=(20, 14))

# 1. Ensemble Score åˆ†ä½ˆ
ax1 = plt.subplot(3, 4, 1)
ax1.hist(ensemble_benign, bins=100, alpha=0.7, label='BENIGN', color='green', density=True)
ax1.hist(ensemble_attack, bins=100, alpha=0.7, label='Attack', color='red', density=True)
ax1.axvline(threshold, color='black', linestyle='--', linewidth=2,
           label=f'Threshold={threshold:.4f}')
ax1.set_xlabel('Ensemble Score')
ax1.set_ylabel('Density')
ax1.set_title('Ensemble Score Distribution', fontweight='bold')
ax1.legend()
ax1.grid(alpha=0.3)

# 2. æ··æ·†çŸ©é™£
ax2 = plt.subplot(3, 4, 2)
cm = np.array([[tn, fp], [fn, tp]])
sns.heatmap(cm, annot=True, fmt=',d', cmap='Blues', ax=ax2,
           xticklabels=['Pred Normal', 'Pred Attack'],
           yticklabels=['True Normal', 'True Attack'])
ax2.set_title('Confusion Matrix', fontweight='bold')

# 3. é–€æª»ç­–ç•¥æ¯”è¼ƒ
ax3 = plt.subplot(3, 4, 3)
strategy_names = [r['name'] for r in threshold_results]
f1_scores = [r['f1'] for r in threshold_results]
colors = ['gold' if r['name'] == threshold_name else 'steelblue' for r in threshold_results]
ax3.barh(strategy_names, f1_scores, color=colors)
ax3.set_xlabel('F1-Score')
ax3.set_title('Threshold Strategy Comparison', fontweight='bold')
ax3.grid(alpha=0.3, axis='x')

# 4. æ€§èƒ½æŒ‡æ¨™é›·é”åœ–
ax4 = plt.subplot(3, 4, 4, projection='polar')
metrics = ['Precision', 'Recall', 'F1', 'Accuracy']
values = [precision, recall, f1, accuracy, precision]
angles = np.linspace(0, 2*np.pi, len(metrics), endpoint=False).tolist()
angles += angles[:1]
ax4.plot(angles, values, 'o-', linewidth=2)
ax4.fill(angles, values, alpha=0.25)
ax4.set_xticks(angles[:-1])
ax4.set_xticklabels(metrics)
ax4.set_ylim(0, 1)
ax4.set_title('Performance Metrics', fontweight='bold', pad=20)
ax4.grid(True)

# 5. å„æ”»æ“Šé¡å‹åµæ¸¬ç‡
ax5 = plt.subplot(3, 4, 5)
if attack_analysis:
    types = [a['type'][:20] for a in attack_analysis]
    rates = [a['rate']*100 for a in attack_analysis]
    colors_bar = ['green' if r >= 80 else 'orange' if r >= 50 else 'red' for r in rates]
    bars = ax5.barh(types, rates, color=colors_bar)
    ax5.set_xlabel('Detection Rate (%)')
    ax5.set_title('Detection Rate by Attack Type', fontweight='bold')
    ax5.set_xlim(0, 105)
    ax5.grid(alpha=0.3, axis='x')
    for i, (bar, rate) in enumerate(zip(bars, rates)):
        ax5.text(rate+2, i, f'{rate:.0f}%', va='center', fontsize=8)

# 6. AE vs RF åˆ†æ•¸æ•£é»åœ–
ax6 = plt.subplot(3, 4, 6)
sample_size = min(5000, len(ae_score_norm))
sample_idx = np.random.choice(len(ae_score_norm), sample_size, replace=False)
colors_scatter = ['red' if l != 'BENIGN' else 'green' for l in labels.iloc[sample_idx]]
ax6.scatter(ae_score_norm[sample_idx], rf_score_norm[sample_idx],
           c=colors_scatter, alpha=0.3, s=5)
ax6.plot([0, 1], [0, 1], 'k--', alpha=0.5)
ax6.set_xlabel('Deep AE Score (norm)')
ax6.set_ylabel('RF Score (norm)')
ax6.set_title('AE vs RF Scores', fontweight='bold')
ax6.grid(alpha=0.3)

# 7. Ensemble Score æ•£é»åœ–
ax7 = plt.subplot(3, 4, 7)
sample_idx2 = np.random.choice(len(ensemble_score), min(5000, len(ensemble_score)), replace=False)
colors_scatter2 = ['red' if l != 'BENIGN' else 'green' for l in labels.iloc[sample_idx2]]
ax7.scatter(sample_idx2, ensemble_score[sample_idx2], c=colors_scatter2, alpha=0.4, s=3)
ax7.axhline(threshold, color='black', linestyle='--', linewidth=2, label='Threshold')
ax7.set_xlabel('Sample Index')
ax7.set_ylabel('Ensemble Score')
ax7.set_title('Ensemble Score Scatter', fontweight='bold')
ax7.legend()
ax7.grid(alpha=0.3)

# 8. æ¼å ±åˆ†æ
ax8 = plt.subplot(3, 4, 8)
if len(false_negatives) > 0:
    fn_scores = ensemble_score[false_negatives_mask]
    ax8.hist(fn_scores, bins=50, alpha=0.7, color='orange', label='False Negatives')
    ax8.hist(ensemble_benign, bins=50, alpha=0.5, color='green', label='BENIGN')
    ax8.axvline(threshold, color='black', linestyle='--', linewidth=2, label='Threshold')
    ax8.set_xlabel('Ensemble Score')
    ax8.set_ylabel('Count')
    ax8.set_title('False Negatives Distribution', fontweight='bold')
    ax8.legend()
    ax8.grid(alpha=0.3)
else:
    ax8.text(0.5, 0.5, 'No False Negatives', ha='center', va='center', fontsize=14,
            fontweight='bold', color='green')
    ax8.set_title('False Negatives Distribution', fontweight='bold')

# 9. èª¤å ±åˆ†æ
ax9 = plt.subplot(3, 4, 9)
if len(false_positives) > 0:
    fp_scores = ensemble_score[false_positives_mask]
    ax9.hist(fp_scores, bins=50, alpha=0.7, color='red', label='False Positives')
    ax9.hist(ensemble_attack, bins=50, alpha=0.5, color='orange', label='Attack')
    ax9.axvline(threshold, color='black', linestyle='--', linewidth=2, label='Threshold')
    ax9.set_xlabel('Ensemble Score')
    ax9.set_ylabel('Count')
    ax9.set_title('False Positives Distribution', fontweight='bold')
    ax9.legend()
    ax9.grid(alpha=0.3)
else:
    ax9.text(0.5, 0.5, 'No False Positives', ha='center', va='center', fontsize=14,
            fontweight='bold', color='green')
    ax9.set_title('False Positives Distribution', fontweight='bold')

# 10. Precision-Recall æ›²ç·š
ax10 = plt.subplot(3, 4, 10)
precisions = [r['precision'] for r in threshold_results]
recalls = [r['recall'] for r in threshold_results]
ax10.plot(recalls, precisions, 'b-o', linewidth=2)
current_idx = [r['name'] for r in threshold_results].index(threshold_name)
ax10.plot(recalls[current_idx], precisions[current_idx], 'r*', markersize=15, label='Current')
ax10.set_xlabel('Recall')
ax10.set_ylabel('Precision')
ax10.set_title('Precision-Recall Curve', fontweight='bold')
ax10.legend()
ax10.grid(alpha=0.3)
ax10.set_xlim(-0.05, 1.05)
ax10.set_ylim(-0.05, 1.05)

# 11. é æ¸¬é¡åˆ¥åˆ†å¸ƒ
ax11 = plt.subplot(3, 4, 11)
if len(pred_labels) > 0:
    pred_counts = pd.Series(pred_labels).value_counts()
    pred_counts.plot(kind='barh', ax=ax11, color='teal')
    ax11.set_title('Predicted Attack Types', fontweight='bold')
    ax11.set_xlabel('Count')
    ax11.grid(alpha=0.3, axis='x')
else:
    ax11.text(0.5, 0.5, 'No Predictions', ha='center', va='center', fontsize=14)
    ax11.set_title('Predicted Attack Types', fontweight='bold')

# 12. MLP æ··æ·†çŸ©é™£
ax12 = plt.subplot(3, 4, 12)
if len(pred_labels) > 0:
    true_labels_of_anomalies = labels.values[is_anomaly == 1]
    mask_real_attack = true_labels_of_anomalies != 'BENIGN'

    if mask_real_attack.sum() > 10:
        true_attack = true_labels_of_anomalies[mask_real_attack]
        pred_attack = pred_labels[mask_real_attack]

        unique_labels = sorted(set(true_attack) | set(pred_attack))
        if len(unique_labels) <= 15:  # åªåœ¨é¡åˆ¥ä¸å¤ªå¤šæ™‚é¡¯ç¤º
            cm_mlp = confusion_matrix(true_attack, pred_attack, labels=unique_labels)
            sns.heatmap(cm_mlp, annot=True, fmt='d', cmap='Greens', ax=ax12,
                       xticklabels=[l[:10] for l in unique_labels],
                       yticklabels=[l[:10] for l in unique_labels])
            ax12.set_title('MLP Classification Matrix', fontweight='bold')
            ax12.set_xlabel('Predicted')
            ax12.set_ylabel('True')
        else:
            ax12.text(0.5, 0.5, f'Too many classes ({len(unique_labels)})',
                     ha='center', va='center')
            ax12.set_title('MLP Classification Matrix', fontweight='bold')
    else:
        ax12.text(0.5, 0.5, 'Insufficient samples', ha='center', va='center')
        ax12.set_title('MLP Classification Matrix', fontweight='bold')
else:
    ax12.text(0.5, 0.5, 'No MLP predictions', ha='center', va='center')
    ax12.set_title('MLP Classification Matrix', fontweight='bold')

plt.tight_layout()
plt.savefig('test_ensemble_analysis.png', dpi=150, bbox_inches='tight')
print("âœ… å·²ä¿å­˜: test_ensemble_analysis.png")

# === 15ï¸âƒ£ ç¸½çµ ===
print("\n" + "=" * 60)
print("âœ… æ¸¬è©¦å®Œæˆï¼")
print("=" * 60)

print(f"""
ğŸ“Š æœ€çµ‚çµæœç¸½çµ:
  æ¸¬è©¦æ¨£æœ¬æ•¸: {len(df):,}
  ä½¿ç”¨é–€æª»: {threshold:.6f} ({threshold_name})
  
ğŸ¯ Ensemble ç•°å¸¸åµæ¸¬:
  åµæ¸¬åˆ°ç•°å¸¸: {is_anomaly.sum():,}
  TPR (Recall): {recall:.2%}
  FPR: {fpr:.2%}
  Precision: {precision:.3f}
  F1-Score: {f1:.3f}
  Accuracy: {accuracy:.3f}
""")

if len(pred_labels) > 0:
    print(f"""
ğŸ§  MLP æ”»æ“Šåˆ†é¡:
  åˆ†é¡æ¨£æœ¬æ•¸: {len(pred_labels):,}
  åˆ†é¡æº–ç¢ºç‡: {classification_acc:.2%}
  (çœŸå¯¦æ”»æ“Š): {attack_acc:.2%}
""")

print("=" * 60)
print("\nğŸ“ è¼¸å‡ºæª”æ¡ˆ:")
print("  - test_ensemble_results.csv")
print("  - test_ensemble_analysis.png")
print("=" * 60)